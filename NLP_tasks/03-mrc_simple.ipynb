{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Transformers的阅读理解实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 10142\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 3219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = load_dataset('cmrc2018', cache_dir='./data')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'TRAIN_186_QUERY_0', 'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。', 'question': '范廷颂是什么时候被任为主教的？', 'answers': {'text': ['1963年'], 'answer_start': [30]}}\n"
     ]
    }
   ],
   "source": [
    "print(datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('D:/pretrained_model/models--hfl--chinese-macbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'TRAIN_186_QUERY_0', 'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。', 'question': '范廷颂是什么时候被任为主教的？', 'answers': {'text': ['1963年'], 'answer_start': [30]}}\n"
     ]
    }
   ],
   "source": [
    "print(datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'answers'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset = datasets['train'].select(range(2))\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_examples = tokenizer(text=sample_dataset['question'],\n",
    "                               text_pair=sample_dataset['context'],\n",
    "                               max_length=512,\n",
    "                               truncation='only_second',\n",
    "                               return_offsets_mapping=True,\n",
    "                               padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_examples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 67), (67, 68), (68, 69), (69, 70), (70, 71), (71, 72), (72, 73), (73, 74), (74, 75), (75, 76), (76, 77), (77, 78), (78, 79), (79, 80), (80, 81), (81, 82), (82, 83), (83, 84), (84, 85), (85, 86), (86, 87), (87, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 100), (100, 101), (101, 105), (105, 106), (106, 107), (107, 108), (108, 110), (110, 111), (111, 112), (112, 113), (113, 114), (114, 115), (115, 116), (116, 117), (117, 118), (118, 119), (119, 120), (120, 121), (121, 122), (122, 123), (123, 124), (124, 125), (125, 126), (126, 127), (127, 128), (128, 129), (129, 130), (130, 131), (131, 132), (132, 133), (133, 134), (134, 135), (135, 136), (136, 137), (137, 138), (138, 139), (139, 140), (140, 141), (141, 142), (142, 143), (143, 144), (144, 145), (145, 146), (146, 147), (147, 148), (148, 149), (149, 150), (150, 151), (151, 152), (152, 153), (153, 154), (154, 155), (155, 156), (156, 157), (157, 158), (158, 159), (159, 163), (163, 164), (164, 165), (165, 166), (166, 167), (167, 168), (168, 169), (169, 170), (170, 171), (171, 172), (172, 173), (173, 174), (174, 175), (175, 176), (176, 177), (177, 178), (178, 179), (179, 180), (180, 181), (181, 182), (182, 186), (186, 187), (187, 188), (188, 189), (189, 190), (190, 191), (191, 192), (192, 193), (193, 194), (194, 195), (195, 196), (196, 197), (197, 198), (198, 199), (199, 200), (200, 201), (201, 202), (202, 203), (203, 204), (204, 205), (205, 206), (206, 207), (207, 208), (208, 209), (209, 210), (210, 211), (211, 212), (212, 213), (213, 214), (214, 215), (215, 216), (216, 217), (217, 218), (218, 222), (222, 223), (223, 224), (224, 225), (225, 226), (226, 227), (227, 228), (228, 229), (229, 230), (230, 231), (231, 232), (232, 233), (233, 234), (234, 235), (235, 236), (236, 237), (237, 238), (238, 239), (239, 240), (240, 241), (241, 242), (242, 243), (243, 244), (244, 245), (245, 246), (246, 247), (247, 248), (248, 249), (249, 250), (250, 251), (251, 252), (252, 253), (253, 257), (257, 258), (258, 259), (259, 260), (260, 261), (261, 262), (262, 263), (263, 264), (264, 265), (265, 266), (266, 267), (267, 268), (268, 269), (269, 270), (270, 271), (271, 272), (272, 273), (273, 274), (274, 275), (275, 276), (276, 277), (277, 278), (278, 279), (279, 280), (280, 281), (281, 282), (282, 283), (283, 284), (284, 285), (285, 286), (286, 287), (287, 288), (288, 289), (289, 290), (290, 291), (291, 292), (292, 293), (293, 294), (294, 295), (295, 296), (296, 297), (297, 298), (298, 299), (299, 300), (300, 301), (301, 302), (302, 303), (303, 304), (304, 305), (305, 306), (306, 307), (307, 308), (308, 309), (309, 310), (310, 311), (311, 312), (312, 313), (313, 314), (314, 315), (315, 316), (316, 317), (317, 318), (318, 319), (319, 320), (320, 321), (321, 325), (325, 326), (326, 327), (327, 328), (328, 329), (329, 330), (330, 331), (331, 332), (332, 333), (333, 334), (334, 335), (335, 336), (336, 337), (337, 338), (338, 339), (339, 340), (340, 341), (341, 342), (342, 343), (343, 344), (344, 345), (345, 346), (346, 347), (347, 348), (348, 349), (349, 350), (350, 351), (351, 352), (352, 353), (353, 354), (354, 355), (355, 356), (356, 360), (360, 361), (361, 362), (362, 363), (363, 364), (364, 365), (365, 366), (366, 367), (367, 368), (368, 369), (369, 370), (370, 371), (371, 372), (372, 373), (373, 374), (374, 375), (375, 376), (376, 377), (377, 378), (378, 379), (379, 380), (380, 381), (381, 382), (382, 383), (383, 384), (384, 385), (385, 386), (386, 387), (387, 388), (388, 390), (390, 391), (391, 392), (392, 393), (393, 394), (394, 395), (395, 396), (396, 397), (397, 398), (398, 399), (399, 400), (400, 401), (401, 402), (402, 403), (403, 404), (404, 405), (405, 406), (406, 407), (407, 408), (408, 409), (409, 410), (410, 411), (411, 412), (412, 413), (413, 414), (414, 415), (415, 416), (416, 417), (417, 418), (418, 419), (419, 420), (420, 421), (421, 422), (422, 424), (424, 425), (425, 426), (426, 427), (427, 428), (428, 429), (429, 430), (430, 431), (431, 432), (432, 433), (433, 434), (434, 435), (435, 436), (436, 437), (437, 438), (438, 439), (439, 440), (440, 441), (441, 442), (442, 443), (443, 444), (444, 445), (445, 446), (446, 447), (447, 448), (448, 449), (449, 450), (450, 451), (451, 452), (452, 453), (453, 454), (454, 455), (455, 456), (456, 457), (457, 458), (458, 459), (459, 460), (460, 461), (461, 462), (462, 463), (463, 464), (464, 465), (465, 466), (466, 467), (467, 468), (468, 469), (469, 470), (470, 471), (471, 472), (472, 473), (473, 474), (474, 475), (475, 476), (476, 477), (477, 478), (478, 479), (479, 480), (480, 481), (481, 482), (482, 483), (483, 484), (484, 485), (485, 486), (486, 487), (487, 488), (488, 489), (489, 490), (490, 491), (491, 492), (492, 493), (493, 494), (494, 495), (495, 499), (499, 500), (500, 501), (501, 502), (502, 503), (503, 504), (504, 505), (505, 506), (506, 507), (507, 508), (508, 509), (509, 510), (510, 511), (511, 512), (512, 513), (513, 514), (514, 516), (516, 517), (517, 518), (518, 519), (519, 520), (520, 521), (521, 522), (522, 523), (523, 524), (524, 525), (525, 526), (526, 527), (527, 528), (528, 529), (529, 530), (530, 531), (531, 532), (532, 533), (533, 534), (0, 0)] 512\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_examples['offset_mapping'][0], len(tokenizer_examples['offset_mapping'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_mapping = tokenizer_examples.pop(\"offset_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 5745, 2455, 7563, 3221, 784, 720, 3198, 952, 6158, 818, 711, 712, 3136, 4638, 8043, 102, 5745, 2455, 7563, 3364, 3322, 8020, 8024, 8021, 8024, 1760, 1399, 924, 4882, 185, 5735, 4449, 8020, 8021, 8024, 3221, 6632, 1298, 5384, 7716, 1921, 712, 3136, 3364, 3322, 511, 9155, 2399, 6158, 818, 711, 712, 3136, 8039, 8431, 2399, 6158, 3091, 1285, 711, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2134, 2429, 5392, 4415, 8039, 8447, 2399, 6158, 3091, 1285, 711, 2600, 712, 3136, 8024, 1398, 2399, 2399, 2419, 6158, 3091, 1285, 711, 3364, 3322, 8039, 8170, 2399, 123, 3299, 4895, 686, 511, 5745, 2455, 7563, 754, 9915, 2399, 127, 3299, 8115, 3189, 1762, 6632, 1298, 2123, 2398, 4689, 1921, 712, 3136, 1355, 5683, 3136, 1277, 1139, 4495, 8039, 4997, 2399, 3198, 2970, 1358, 5679, 1962, 3136, 5509, 1400, 8024, 6158, 671, 855, 6632, 1298, 4868, 4266, 2372, 1168, 3777, 1079, 5326, 5330, 1071, 2110, 689, 511, 5745, 2455, 7563, 754, 9211, 2399, 1762, 3777, 1079, 1920, 934, 6887, 7368, 2130, 2768, 4868, 2110, 2110, 689, 511, 5745, 2455, 7563, 754, 8594, 2399, 127, 3299, 127, 3189, 1762, 3777, 1079, 4638, 712, 3136, 2429, 1828, 3232, 7195, 8039, 1350, 1400, 6158, 3836, 1168, 1760, 1957, 2207, 2548, 1065, 2109, 1036, 7368, 3302, 1218, 511, 8707, 2399, 807, 8024, 5745, 2455, 7563, 1762, 3777, 1079, 1828, 1277, 1158, 2456, 4919, 3696, 2970, 2521, 704, 2552, 809, 3119, 2159, 1168, 3777, 1079, 6912, 2773, 4638, 7410, 3696, 511, 9258, 2399, 8024, 3791, 6632, 2773, 751, 5310, 3338, 8024, 6632, 1298, 3696, 712, 1066, 1469, 1744, 2456, 6963, 3777, 1079, 8024, 2496, 3198, 2523, 1914, 1921, 712, 3136, 4868, 5466, 782, 1447, 6845, 5635, 6632, 1298, 4638, 1298, 3175, 8024, 852, 5745, 2455, 7563, 793, 4197, 4522, 1762, 3777, 1079, 511, 5422, 2399, 5052, 4415, 1760, 5735, 3307, 2207, 934, 7368, 8039, 2668, 1762, 8779, 2399, 1728, 2932, 1310, 934, 7368, 4638, 5632, 4507, 510, 5632, 3780, 1350, 2867, 5318, 3124, 2424, 1762, 934, 7368, 6392, 3124, 3780, 6440, 4638, 6206, 3724, 5445, 6158, 2936, 511, 9155, 2399, 125, 3299, 126, 3189, 8024, 3136, 2134, 818, 1462, 5745, 2455, 7563, 711, 1921, 712, 3136, 1266, 2123, 3136, 1277, 712, 3136, 8024, 1398, 2399, 129, 3299, 8115, 3189, 2218, 818, 8039, 1071, 4288, 7208, 711, 519, 2769, 928, 1921, 712, 4638, 4263, 520, 511, 4507, 754, 5745, 2455, 7563, 6158, 6632, 1298, 3124, 2424, 6763, 4881, 2345, 679, 1914, 8114, 2399, 8024, 1728, 3634, 800, 3187, 3791, 1168, 2792, 2247, 1828, 1277, 6822, 6121, 4288, 4130, 2339, 868, 5445, 683, 3800, 4777, 6438, 5023, 2339, 868, 511, 5745, 2455, 7563, 7370, 749, 7481, 2190, 2773, 751, 510, 6577, 1737, 510, 6158, 2496, 2229, 6833, 2154, 1921, 712, 3136, 833, 5023, 7309, 7579, 1912, 8024, 738, 4908, 2166, 2612, 1908, 934, 7368, 510, 1158, 2456, 1957, 934, 833, 1730, 860, 5023, 511, 8431, 2399, 8024, 3136, 2134, 5735, 3307, 924, 4882, 753, 686, 1762, 1398, 2399, 127, 3299, 8123, 3189, 3091, 1285, 5745, 2455, 7563, 711, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2134, 2429, 5392, 102], [101, 8431, 2399, 8024, 5745, 2455, 7563, 2857, 818, 784, 720, 5466, 1218, 8043, 102, 5745, 2455, 7563, 3364, 3322, 8020, 8024, 8021, 8024, 1760, 1399, 924, 4882, 185, 5735, 4449, 8020, 8021, 8024, 3221, 6632, 1298, 5384, 7716, 1921, 712, 3136, 3364, 3322, 511, 9155, 2399, 6158, 818, 711, 712, 3136, 8039, 8431, 2399, 6158, 3091, 1285, 711, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2134, 2429, 5392, 4415, 8039, 8447, 2399, 6158, 3091, 1285, 711, 2600, 712, 3136, 8024, 1398, 2399, 2399, 2419, 6158, 3091, 1285, 711, 3364, 3322, 8039, 8170, 2399, 123, 3299, 4895, 686, 511, 5745, 2455, 7563, 754, 9915, 2399, 127, 3299, 8115, 3189, 1762, 6632, 1298, 2123, 2398, 4689, 1921, 712, 3136, 1355, 5683, 3136, 1277, 1139, 4495, 8039, 4997, 2399, 3198, 2970, 1358, 5679, 1962, 3136, 5509, 1400, 8024, 6158, 671, 855, 6632, 1298, 4868, 4266, 2372, 1168, 3777, 1079, 5326, 5330, 1071, 2110, 689, 511, 5745, 2455, 7563, 754, 9211, 2399, 1762, 3777, 1079, 1920, 934, 6887, 7368, 2130, 2768, 4868, 2110, 2110, 689, 511, 5745, 2455, 7563, 754, 8594, 2399, 127, 3299, 127, 3189, 1762, 3777, 1079, 4638, 712, 3136, 2429, 1828, 3232, 7195, 8039, 1350, 1400, 6158, 3836, 1168, 1760, 1957, 2207, 2548, 1065, 2109, 1036, 7368, 3302, 1218, 511, 8707, 2399, 807, 8024, 5745, 2455, 7563, 1762, 3777, 1079, 1828, 1277, 1158, 2456, 4919, 3696, 2970, 2521, 704, 2552, 809, 3119, 2159, 1168, 3777, 1079, 6912, 2773, 4638, 7410, 3696, 511, 9258, 2399, 8024, 3791, 6632, 2773, 751, 5310, 3338, 8024, 6632, 1298, 3696, 712, 1066, 1469, 1744, 2456, 6963, 3777, 1079, 8024, 2496, 3198, 2523, 1914, 1921, 712, 3136, 4868, 5466, 782, 1447, 6845, 5635, 6632, 1298, 4638, 1298, 3175, 8024, 852, 5745, 2455, 7563, 793, 4197, 4522, 1762, 3777, 1079, 511, 5422, 2399, 5052, 4415, 1760, 5735, 3307, 2207, 934, 7368, 8039, 2668, 1762, 8779, 2399, 1728, 2932, 1310, 934, 7368, 4638, 5632, 4507, 510, 5632, 3780, 1350, 2867, 5318, 3124, 2424, 1762, 934, 7368, 6392, 3124, 3780, 6440, 4638, 6206, 3724, 5445, 6158, 2936, 511, 9155, 2399, 125, 3299, 126, 3189, 8024, 3136, 2134, 818, 1462, 5745, 2455, 7563, 711, 1921, 712, 3136, 1266, 2123, 3136, 1277, 712, 3136, 8024, 1398, 2399, 129, 3299, 8115, 3189, 2218, 818, 8039, 1071, 4288, 7208, 711, 519, 2769, 928, 1921, 712, 4638, 4263, 520, 511, 4507, 754, 5745, 2455, 7563, 6158, 6632, 1298, 3124, 2424, 6763, 4881, 2345, 679, 1914, 8114, 2399, 8024, 1728, 3634, 800, 3187, 3791, 1168, 2792, 2247, 1828, 1277, 6822, 6121, 4288, 4130, 2339, 868, 5445, 683, 3800, 4777, 6438, 5023, 2339, 868, 511, 5745, 2455, 7563, 7370, 749, 7481, 2190, 2773, 751, 510, 6577, 1737, 510, 6158, 2496, 2229, 6833, 2154, 1921, 712, 3136, 833, 5023, 7309, 7579, 1912, 8024, 738, 4908, 2166, 2612, 1908, 934, 7368, 510, 1158, 2456, 1957, 934, 833, 1730, 860, 5023, 511, 8431, 2399, 8024, 3136, 2134, 5735, 3307, 924, 4882, 753, 686, 1762, 1398, 2399, 127, 3299, 8123, 3189, 3091, 1285, 5745, 2455, 7563, 711, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2134, 2429, 5392, 4415, 809, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " None,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_examples.sequence_ids(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[(0, 0),\n",
       "   (0, 1),\n",
       "   (1, 2),\n",
       "   (2, 3),\n",
       "   (3, 4),\n",
       "   (4, 5),\n",
       "   (5, 6),\n",
       "   (6, 7),\n",
       "   (7, 8),\n",
       "   (8, 9),\n",
       "   (9, 10),\n",
       "   (10, 11),\n",
       "   (11, 12),\n",
       "   (12, 13),\n",
       "   (13, 14),\n",
       "   (14, 15),\n",
       "   (0, 0),\n",
       "   (0, 1),\n",
       "   (1, 2),\n",
       "   (2, 3),\n",
       "   (3, 4),\n",
       "   (4, 5),\n",
       "   (5, 6),\n",
       "   (6, 7),\n",
       "   (7, 8),\n",
       "   (8, 9),\n",
       "   (9, 10),\n",
       "   (10, 11),\n",
       "   (11, 12),\n",
       "   (12, 13),\n",
       "   (13, 14),\n",
       "   (14, 15),\n",
       "   (15, 16),\n",
       "   (16, 17),\n",
       "   (17, 18),\n",
       "   (18, 19),\n",
       "   (19, 20),\n",
       "   (20, 21),\n",
       "   (21, 22),\n",
       "   (22, 23),\n",
       "   (23, 24),\n",
       "   (24, 25),\n",
       "   (25, 26),\n",
       "   (26, 27),\n",
       "   (27, 28),\n",
       "   (28, 29),\n",
       "   (29, 30),\n",
       "   (30, 34),\n",
       "   (34, 35),\n",
       "   (35, 36),\n",
       "   (36, 37),\n",
       "   (37, 38),\n",
       "   (38, 39),\n",
       "   (39, 40),\n",
       "   (40, 41),\n",
       "   (41, 45),\n",
       "   (45, 46),\n",
       "   (46, 47),\n",
       "   (47, 48),\n",
       "   (48, 49),\n",
       "   (49, 50),\n",
       "   (50, 51),\n",
       "   (51, 52),\n",
       "   (52, 53),\n",
       "   (53, 54),\n",
       "   (54, 55),\n",
       "   (55, 56),\n",
       "   (56, 57),\n",
       "   (57, 58),\n",
       "   (58, 59),\n",
       "   (59, 60),\n",
       "   (60, 61),\n",
       "   (61, 62),\n",
       "   (62, 63),\n",
       "   (63, 67),\n",
       "   (67, 68),\n",
       "   (68, 69),\n",
       "   (69, 70),\n",
       "   (70, 71),\n",
       "   (71, 72),\n",
       "   (72, 73),\n",
       "   (73, 74),\n",
       "   (74, 75),\n",
       "   (75, 76),\n",
       "   (76, 77),\n",
       "   (77, 78),\n",
       "   (78, 79),\n",
       "   (79, 80),\n",
       "   (80, 81),\n",
       "   (81, 82),\n",
       "   (82, 83),\n",
       "   (83, 84),\n",
       "   (84, 85),\n",
       "   (85, 86),\n",
       "   (86, 87),\n",
       "   (87, 91),\n",
       "   (91, 92),\n",
       "   (92, 93),\n",
       "   (93, 94),\n",
       "   (94, 95),\n",
       "   (95, 96),\n",
       "   (96, 97),\n",
       "   (97, 98),\n",
       "   (98, 99),\n",
       "   (99, 100),\n",
       "   (100, 101),\n",
       "   (101, 105),\n",
       "   (105, 106),\n",
       "   (106, 107),\n",
       "   (107, 108),\n",
       "   (108, 110),\n",
       "   (110, 111),\n",
       "   (111, 112),\n",
       "   (112, 113),\n",
       "   (113, 114),\n",
       "   (114, 115),\n",
       "   (115, 116),\n",
       "   (116, 117),\n",
       "   (117, 118),\n",
       "   (118, 119),\n",
       "   (119, 120),\n",
       "   (120, 121),\n",
       "   (121, 122),\n",
       "   (122, 123),\n",
       "   (123, 124),\n",
       "   (124, 125),\n",
       "   (125, 126),\n",
       "   (126, 127),\n",
       "   (127, 128),\n",
       "   (128, 129),\n",
       "   (129, 130),\n",
       "   (130, 131),\n",
       "   (131, 132),\n",
       "   (132, 133),\n",
       "   (133, 134),\n",
       "   (134, 135),\n",
       "   (135, 136),\n",
       "   (136, 137),\n",
       "   (137, 138),\n",
       "   (138, 139),\n",
       "   (139, 140),\n",
       "   (140, 141),\n",
       "   (141, 142),\n",
       "   (142, 143),\n",
       "   (143, 144),\n",
       "   (144, 145),\n",
       "   (145, 146),\n",
       "   (146, 147),\n",
       "   (147, 148),\n",
       "   (148, 149),\n",
       "   (149, 150),\n",
       "   (150, 151),\n",
       "   (151, 152),\n",
       "   (152, 153),\n",
       "   (153, 154),\n",
       "   (154, 155),\n",
       "   (155, 156),\n",
       "   (156, 157),\n",
       "   (157, 158),\n",
       "   (158, 159),\n",
       "   (159, 163),\n",
       "   (163, 164),\n",
       "   (164, 165),\n",
       "   (165, 166),\n",
       "   (166, 167),\n",
       "   (167, 168),\n",
       "   (168, 169),\n",
       "   (169, 170),\n",
       "   (170, 171),\n",
       "   (171, 172),\n",
       "   (172, 173),\n",
       "   (173, 174),\n",
       "   (174, 175),\n",
       "   (175, 176),\n",
       "   (176, 177),\n",
       "   (177, 178),\n",
       "   (178, 179),\n",
       "   (179, 180),\n",
       "   (180, 181),\n",
       "   (181, 182),\n",
       "   (182, 186),\n",
       "   (186, 187),\n",
       "   (187, 188),\n",
       "   (188, 189),\n",
       "   (189, 190),\n",
       "   (190, 191),\n",
       "   (191, 192),\n",
       "   (192, 193),\n",
       "   (193, 194),\n",
       "   (194, 195),\n",
       "   (195, 196),\n",
       "   (196, 197),\n",
       "   (197, 198),\n",
       "   (198, 199),\n",
       "   (199, 200),\n",
       "   (200, 201),\n",
       "   (201, 202),\n",
       "   (202, 203),\n",
       "   (203, 204),\n",
       "   (204, 205),\n",
       "   (205, 206),\n",
       "   (206, 207),\n",
       "   (207, 208),\n",
       "   (208, 209),\n",
       "   (209, 210),\n",
       "   (210, 211),\n",
       "   (211, 212),\n",
       "   (212, 213),\n",
       "   (213, 214),\n",
       "   (214, 215),\n",
       "   (215, 216),\n",
       "   (216, 217),\n",
       "   (217, 218),\n",
       "   (218, 222),\n",
       "   (222, 223),\n",
       "   (223, 224),\n",
       "   (224, 225),\n",
       "   (225, 226),\n",
       "   (226, 227),\n",
       "   (227, 228),\n",
       "   (228, 229),\n",
       "   (229, 230),\n",
       "   (230, 231),\n",
       "   (231, 232),\n",
       "   (232, 233),\n",
       "   (233, 234),\n",
       "   (234, 235),\n",
       "   (235, 236),\n",
       "   (236, 237),\n",
       "   (237, 238),\n",
       "   (238, 239),\n",
       "   (239, 240),\n",
       "   (240, 241),\n",
       "   (241, 242),\n",
       "   (242, 243),\n",
       "   (243, 244),\n",
       "   (244, 245),\n",
       "   (245, 246),\n",
       "   (246, 247),\n",
       "   (247, 248),\n",
       "   (248, 249),\n",
       "   (249, 250),\n",
       "   (250, 251),\n",
       "   (251, 252),\n",
       "   (252, 253),\n",
       "   (253, 257),\n",
       "   (257, 258),\n",
       "   (258, 259),\n",
       "   (259, 260),\n",
       "   (260, 261),\n",
       "   (261, 262),\n",
       "   (262, 263),\n",
       "   (263, 264),\n",
       "   (264, 265),\n",
       "   (265, 266),\n",
       "   (266, 267),\n",
       "   (267, 268),\n",
       "   (268, 269),\n",
       "   (269, 270),\n",
       "   (270, 271),\n",
       "   (271, 272),\n",
       "   (272, 273),\n",
       "   (273, 274),\n",
       "   (274, 275),\n",
       "   (275, 276),\n",
       "   (276, 277),\n",
       "   (277, 278),\n",
       "   (278, 279),\n",
       "   (279, 280),\n",
       "   (280, 281),\n",
       "   (281, 282),\n",
       "   (282, 283),\n",
       "   (283, 284),\n",
       "   (284, 285),\n",
       "   (285, 286),\n",
       "   (286, 287),\n",
       "   (287, 288),\n",
       "   (288, 289),\n",
       "   (289, 290),\n",
       "   (290, 291),\n",
       "   (291, 292),\n",
       "   (292, 293),\n",
       "   (293, 294),\n",
       "   (294, 295),\n",
       "   (295, 296),\n",
       "   (296, 297),\n",
       "   (297, 298),\n",
       "   (298, 299),\n",
       "   (299, 300),\n",
       "   (300, 301),\n",
       "   (301, 302),\n",
       "   (302, 303),\n",
       "   (303, 304),\n",
       "   (304, 305),\n",
       "   (305, 306),\n",
       "   (306, 307),\n",
       "   (307, 308),\n",
       "   (308, 309),\n",
       "   (309, 310),\n",
       "   (310, 311),\n",
       "   (311, 312),\n",
       "   (312, 313),\n",
       "   (313, 314),\n",
       "   (314, 315),\n",
       "   (315, 316),\n",
       "   (316, 317),\n",
       "   (317, 318),\n",
       "   (318, 319),\n",
       "   (319, 320),\n",
       "   (320, 321),\n",
       "   (321, 325),\n",
       "   (325, 326),\n",
       "   (326, 327),\n",
       "   (327, 328),\n",
       "   (328, 329),\n",
       "   (329, 330),\n",
       "   (330, 331),\n",
       "   (331, 332),\n",
       "   (332, 333),\n",
       "   (333, 334),\n",
       "   (334, 335),\n",
       "   (335, 336),\n",
       "   (336, 337),\n",
       "   (337, 338),\n",
       "   (338, 339),\n",
       "   (339, 340),\n",
       "   (340, 341),\n",
       "   (341, 342),\n",
       "   (342, 343),\n",
       "   (343, 344),\n",
       "   (344, 345),\n",
       "   (345, 346),\n",
       "   (346, 347),\n",
       "   (347, 348),\n",
       "   (348, 349),\n",
       "   (349, 350),\n",
       "   (350, 351),\n",
       "   (351, 352),\n",
       "   (352, 353),\n",
       "   (353, 354),\n",
       "   (354, 355),\n",
       "   (355, 356),\n",
       "   (356, 360),\n",
       "   (360, 361),\n",
       "   (361, 362),\n",
       "   (362, 363),\n",
       "   (363, 364),\n",
       "   (364, 365),\n",
       "   (365, 366),\n",
       "   (366, 367),\n",
       "   (367, 368),\n",
       "   (368, 369),\n",
       "   (369, 370),\n",
       "   (370, 371),\n",
       "   (371, 372),\n",
       "   (372, 373),\n",
       "   (373, 374),\n",
       "   (374, 375),\n",
       "   (375, 376),\n",
       "   (376, 377),\n",
       "   (377, 378),\n",
       "   (378, 379),\n",
       "   (379, 380),\n",
       "   (380, 381),\n",
       "   (381, 382),\n",
       "   (382, 383),\n",
       "   (383, 384),\n",
       "   (384, 385),\n",
       "   (385, 386),\n",
       "   (386, 387),\n",
       "   (387, 388),\n",
       "   (388, 390),\n",
       "   (390, 391),\n",
       "   (391, 392),\n",
       "   (392, 393),\n",
       "   (393, 394),\n",
       "   (394, 395),\n",
       "   (395, 396),\n",
       "   (396, 397),\n",
       "   (397, 398),\n",
       "   (398, 399),\n",
       "   (399, 400),\n",
       "   (400, 401),\n",
       "   (401, 402),\n",
       "   (402, 403),\n",
       "   (403, 404),\n",
       "   (404, 405),\n",
       "   (405, 406),\n",
       "   (406, 407),\n",
       "   (407, 408),\n",
       "   (408, 409),\n",
       "   (409, 410),\n",
       "   (410, 411),\n",
       "   (411, 412),\n",
       "   (412, 413),\n",
       "   (413, 414),\n",
       "   (414, 415),\n",
       "   (415, 416),\n",
       "   (416, 417),\n",
       "   (417, 418),\n",
       "   (418, 419),\n",
       "   (419, 420),\n",
       "   (420, 421),\n",
       "   (421, 422),\n",
       "   (422, 424),\n",
       "   (424, 425),\n",
       "   (425, 426),\n",
       "   (426, 427),\n",
       "   (427, 428),\n",
       "   (428, 429),\n",
       "   (429, 430),\n",
       "   (430, 431),\n",
       "   (431, 432),\n",
       "   (432, 433),\n",
       "   (433, 434),\n",
       "   (434, 435),\n",
       "   (435, 436),\n",
       "   (436, 437),\n",
       "   (437, 438),\n",
       "   (438, 439),\n",
       "   (439, 440),\n",
       "   (440, 441),\n",
       "   (441, 442),\n",
       "   (442, 443),\n",
       "   (443, 444),\n",
       "   (444, 445),\n",
       "   (445, 446),\n",
       "   (446, 447),\n",
       "   (447, 448),\n",
       "   (448, 449),\n",
       "   (449, 450),\n",
       "   (450, 451),\n",
       "   (451, 452),\n",
       "   (452, 453),\n",
       "   (453, 454),\n",
       "   (454, 455),\n",
       "   (455, 456),\n",
       "   (456, 457),\n",
       "   (457, 458),\n",
       "   (458, 459),\n",
       "   (459, 460),\n",
       "   (460, 461),\n",
       "   (461, 462),\n",
       "   (462, 463),\n",
       "   (463, 464),\n",
       "   (464, 465),\n",
       "   (465, 466),\n",
       "   (466, 467),\n",
       "   (467, 468),\n",
       "   (468, 469),\n",
       "   (469, 470),\n",
       "   (470, 471),\n",
       "   (471, 472),\n",
       "   (472, 473),\n",
       "   (473, 474),\n",
       "   (474, 475),\n",
       "   (475, 476),\n",
       "   (476, 477),\n",
       "   (477, 478),\n",
       "   (478, 479),\n",
       "   (479, 480),\n",
       "   (480, 481),\n",
       "   (481, 482),\n",
       "   (482, 483),\n",
       "   (483, 484),\n",
       "   (484, 485),\n",
       "   (485, 486),\n",
       "   (486, 487),\n",
       "   (487, 488),\n",
       "   (488, 489),\n",
       "   (489, 490),\n",
       "   (490, 491),\n",
       "   (491, 492),\n",
       "   (492, 493),\n",
       "   (493, 494),\n",
       "   (494, 495),\n",
       "   (495, 499),\n",
       "   (499, 500),\n",
       "   (500, 501),\n",
       "   (501, 502),\n",
       "   (502, 503),\n",
       "   (503, 504),\n",
       "   (504, 505),\n",
       "   (505, 506),\n",
       "   (506, 507),\n",
       "   (507, 508),\n",
       "   (508, 509),\n",
       "   (509, 510),\n",
       "   (510, 511),\n",
       "   (511, 512),\n",
       "   (512, 513),\n",
       "   (513, 514),\n",
       "   (514, 516),\n",
       "   (516, 517),\n",
       "   (517, 518),\n",
       "   (518, 519),\n",
       "   (519, 520),\n",
       "   (520, 521),\n",
       "   (521, 522),\n",
       "   (522, 523),\n",
       "   (523, 524),\n",
       "   (524, 525),\n",
       "   (525, 526),\n",
       "   (526, 527),\n",
       "   (527, 528),\n",
       "   (528, 529),\n",
       "   (529, 530),\n",
       "   (530, 531),\n",
       "   (531, 532),\n",
       "   (532, 533),\n",
       "   (533, 534),\n",
       "   (0, 0)],\n",
       "  [(0, 0),\n",
       "   (0, 4),\n",
       "   (4, 5),\n",
       "   (5, 6),\n",
       "   (6, 7),\n",
       "   (7, 8),\n",
       "   (8, 9),\n",
       "   (9, 10),\n",
       "   (10, 11),\n",
       "   (11, 12),\n",
       "   (12, 13),\n",
       "   (13, 14),\n",
       "   (14, 15),\n",
       "   (15, 16),\n",
       "   (0, 0),\n",
       "   (0, 1),\n",
       "   (1, 2),\n",
       "   (2, 3),\n",
       "   (3, 4),\n",
       "   (4, 5),\n",
       "   (5, 6),\n",
       "   (6, 7),\n",
       "   (7, 8),\n",
       "   (8, 9),\n",
       "   (9, 10),\n",
       "   (10, 11),\n",
       "   (11, 12),\n",
       "   (12, 13),\n",
       "   (13, 14),\n",
       "   (14, 15),\n",
       "   (15, 16),\n",
       "   (16, 17),\n",
       "   (17, 18),\n",
       "   (18, 19),\n",
       "   (19, 20),\n",
       "   (20, 21),\n",
       "   (21, 22),\n",
       "   (22, 23),\n",
       "   (23, 24),\n",
       "   (24, 25),\n",
       "   (25, 26),\n",
       "   (26, 27),\n",
       "   (27, 28),\n",
       "   (28, 29),\n",
       "   (29, 30),\n",
       "   (30, 34),\n",
       "   (34, 35),\n",
       "   (35, 36),\n",
       "   (36, 37),\n",
       "   (37, 38),\n",
       "   (38, 39),\n",
       "   (39, 40),\n",
       "   (40, 41),\n",
       "   (41, 45),\n",
       "   (45, 46),\n",
       "   (46, 47),\n",
       "   (47, 48),\n",
       "   (48, 49),\n",
       "   (49, 50),\n",
       "   (50, 51),\n",
       "   (51, 52),\n",
       "   (52, 53),\n",
       "   (53, 54),\n",
       "   (54, 55),\n",
       "   (55, 56),\n",
       "   (56, 57),\n",
       "   (57, 58),\n",
       "   (58, 59),\n",
       "   (59, 60),\n",
       "   (60, 61),\n",
       "   (61, 62),\n",
       "   (62, 63),\n",
       "   (63, 67),\n",
       "   (67, 68),\n",
       "   (68, 69),\n",
       "   (69, 70),\n",
       "   (70, 71),\n",
       "   (71, 72),\n",
       "   (72, 73),\n",
       "   (73, 74),\n",
       "   (74, 75),\n",
       "   (75, 76),\n",
       "   (76, 77),\n",
       "   (77, 78),\n",
       "   (78, 79),\n",
       "   (79, 80),\n",
       "   (80, 81),\n",
       "   (81, 82),\n",
       "   (82, 83),\n",
       "   (83, 84),\n",
       "   (84, 85),\n",
       "   (85, 86),\n",
       "   (86, 87),\n",
       "   (87, 91),\n",
       "   (91, 92),\n",
       "   (92, 93),\n",
       "   (93, 94),\n",
       "   (94, 95),\n",
       "   (95, 96),\n",
       "   (96, 97),\n",
       "   (97, 98),\n",
       "   (98, 99),\n",
       "   (99, 100),\n",
       "   (100, 101),\n",
       "   (101, 105),\n",
       "   (105, 106),\n",
       "   (106, 107),\n",
       "   (107, 108),\n",
       "   (108, 110),\n",
       "   (110, 111),\n",
       "   (111, 112),\n",
       "   (112, 113),\n",
       "   (113, 114),\n",
       "   (114, 115),\n",
       "   (115, 116),\n",
       "   (116, 117),\n",
       "   (117, 118),\n",
       "   (118, 119),\n",
       "   (119, 120),\n",
       "   (120, 121),\n",
       "   (121, 122),\n",
       "   (122, 123),\n",
       "   (123, 124),\n",
       "   (124, 125),\n",
       "   (125, 126),\n",
       "   (126, 127),\n",
       "   (127, 128),\n",
       "   (128, 129),\n",
       "   (129, 130),\n",
       "   (130, 131),\n",
       "   (131, 132),\n",
       "   (132, 133),\n",
       "   (133, 134),\n",
       "   (134, 135),\n",
       "   (135, 136),\n",
       "   (136, 137),\n",
       "   (137, 138),\n",
       "   (138, 139),\n",
       "   (139, 140),\n",
       "   (140, 141),\n",
       "   (141, 142),\n",
       "   (142, 143),\n",
       "   (143, 144),\n",
       "   (144, 145),\n",
       "   (145, 146),\n",
       "   (146, 147),\n",
       "   (147, 148),\n",
       "   (148, 149),\n",
       "   (149, 150),\n",
       "   (150, 151),\n",
       "   (151, 152),\n",
       "   (152, 153),\n",
       "   (153, 154),\n",
       "   (154, 155),\n",
       "   (155, 156),\n",
       "   (156, 157),\n",
       "   (157, 158),\n",
       "   (158, 159),\n",
       "   (159, 163),\n",
       "   (163, 164),\n",
       "   (164, 165),\n",
       "   (165, 166),\n",
       "   (166, 167),\n",
       "   (167, 168),\n",
       "   (168, 169),\n",
       "   (169, 170),\n",
       "   (170, 171),\n",
       "   (171, 172),\n",
       "   (172, 173),\n",
       "   (173, 174),\n",
       "   (174, 175),\n",
       "   (175, 176),\n",
       "   (176, 177),\n",
       "   (177, 178),\n",
       "   (178, 179),\n",
       "   (179, 180),\n",
       "   (180, 181),\n",
       "   (181, 182),\n",
       "   (182, 186),\n",
       "   (186, 187),\n",
       "   (187, 188),\n",
       "   (188, 189),\n",
       "   (189, 190),\n",
       "   (190, 191),\n",
       "   (191, 192),\n",
       "   (192, 193),\n",
       "   (193, 194),\n",
       "   (194, 195),\n",
       "   (195, 196),\n",
       "   (196, 197),\n",
       "   (197, 198),\n",
       "   (198, 199),\n",
       "   (199, 200),\n",
       "   (200, 201),\n",
       "   (201, 202),\n",
       "   (202, 203),\n",
       "   (203, 204),\n",
       "   (204, 205),\n",
       "   (205, 206),\n",
       "   (206, 207),\n",
       "   (207, 208),\n",
       "   (208, 209),\n",
       "   (209, 210),\n",
       "   (210, 211),\n",
       "   (211, 212),\n",
       "   (212, 213),\n",
       "   (213, 214),\n",
       "   (214, 215),\n",
       "   (215, 216),\n",
       "   (216, 217),\n",
       "   (217, 218),\n",
       "   (218, 222),\n",
       "   (222, 223),\n",
       "   (223, 224),\n",
       "   (224, 225),\n",
       "   (225, 226),\n",
       "   (226, 227),\n",
       "   (227, 228),\n",
       "   (228, 229),\n",
       "   (229, 230),\n",
       "   (230, 231),\n",
       "   (231, 232),\n",
       "   (232, 233),\n",
       "   (233, 234),\n",
       "   (234, 235),\n",
       "   (235, 236),\n",
       "   (236, 237),\n",
       "   (237, 238),\n",
       "   (238, 239),\n",
       "   (239, 240),\n",
       "   (240, 241),\n",
       "   (241, 242),\n",
       "   (242, 243),\n",
       "   (243, 244),\n",
       "   (244, 245),\n",
       "   (245, 246),\n",
       "   (246, 247),\n",
       "   (247, 248),\n",
       "   (248, 249),\n",
       "   (249, 250),\n",
       "   (250, 251),\n",
       "   (251, 252),\n",
       "   (252, 253),\n",
       "   (253, 257),\n",
       "   (257, 258),\n",
       "   (258, 259),\n",
       "   (259, 260),\n",
       "   (260, 261),\n",
       "   (261, 262),\n",
       "   (262, 263),\n",
       "   (263, 264),\n",
       "   (264, 265),\n",
       "   (265, 266),\n",
       "   (266, 267),\n",
       "   (267, 268),\n",
       "   (268, 269),\n",
       "   (269, 270),\n",
       "   (270, 271),\n",
       "   (271, 272),\n",
       "   (272, 273),\n",
       "   (273, 274),\n",
       "   (274, 275),\n",
       "   (275, 276),\n",
       "   (276, 277),\n",
       "   (277, 278),\n",
       "   (278, 279),\n",
       "   (279, 280),\n",
       "   (280, 281),\n",
       "   (281, 282),\n",
       "   (282, 283),\n",
       "   (283, 284),\n",
       "   (284, 285),\n",
       "   (285, 286),\n",
       "   (286, 287),\n",
       "   (287, 288),\n",
       "   (288, 289),\n",
       "   (289, 290),\n",
       "   (290, 291),\n",
       "   (291, 292),\n",
       "   (292, 293),\n",
       "   (293, 294),\n",
       "   (294, 295),\n",
       "   (295, 296),\n",
       "   (296, 297),\n",
       "   (297, 298),\n",
       "   (298, 299),\n",
       "   (299, 300),\n",
       "   (300, 301),\n",
       "   (301, 302),\n",
       "   (302, 303),\n",
       "   (303, 304),\n",
       "   (304, 305),\n",
       "   (305, 306),\n",
       "   (306, 307),\n",
       "   (307, 308),\n",
       "   (308, 309),\n",
       "   (309, 310),\n",
       "   (310, 311),\n",
       "   (311, 312),\n",
       "   (312, 313),\n",
       "   (313, 314),\n",
       "   (314, 315),\n",
       "   (315, 316),\n",
       "   (316, 317),\n",
       "   (317, 318),\n",
       "   (318, 319),\n",
       "   (319, 320),\n",
       "   (320, 321),\n",
       "   (321, 325),\n",
       "   (325, 326),\n",
       "   (326, 327),\n",
       "   (327, 328),\n",
       "   (328, 329),\n",
       "   (329, 330),\n",
       "   (330, 331),\n",
       "   (331, 332),\n",
       "   (332, 333),\n",
       "   (333, 334),\n",
       "   (334, 335),\n",
       "   (335, 336),\n",
       "   (336, 337),\n",
       "   (337, 338),\n",
       "   (338, 339),\n",
       "   (339, 340),\n",
       "   (340, 341),\n",
       "   (341, 342),\n",
       "   (342, 343),\n",
       "   (343, 344),\n",
       "   (344, 345),\n",
       "   (345, 346),\n",
       "   (346, 347),\n",
       "   (347, 348),\n",
       "   (348, 349),\n",
       "   (349, 350),\n",
       "   (350, 351),\n",
       "   (351, 352),\n",
       "   (352, 353),\n",
       "   (353, 354),\n",
       "   (354, 355),\n",
       "   (355, 356),\n",
       "   (356, 360),\n",
       "   (360, 361),\n",
       "   (361, 362),\n",
       "   (362, 363),\n",
       "   (363, 364),\n",
       "   (364, 365),\n",
       "   (365, 366),\n",
       "   (366, 367),\n",
       "   (367, 368),\n",
       "   (368, 369),\n",
       "   (369, 370),\n",
       "   (370, 371),\n",
       "   (371, 372),\n",
       "   (372, 373),\n",
       "   (373, 374),\n",
       "   (374, 375),\n",
       "   (375, 376),\n",
       "   (376, 377),\n",
       "   (377, 378),\n",
       "   (378, 379),\n",
       "   (379, 380),\n",
       "   (380, 381),\n",
       "   (381, 382),\n",
       "   (382, 383),\n",
       "   (383, 384),\n",
       "   (384, 385),\n",
       "   (385, 386),\n",
       "   (386, 387),\n",
       "   (387, 388),\n",
       "   (388, 390),\n",
       "   (390, 391),\n",
       "   (391, 392),\n",
       "   (392, 393),\n",
       "   (393, 394),\n",
       "   (394, 395),\n",
       "   (395, 396),\n",
       "   (396, 397),\n",
       "   (397, 398),\n",
       "   (398, 399),\n",
       "   (399, 400),\n",
       "   (400, 401),\n",
       "   (401, 402),\n",
       "   (402, 403),\n",
       "   (403, 404),\n",
       "   (404, 405),\n",
       "   (405, 406),\n",
       "   (406, 407),\n",
       "   (407, 408),\n",
       "   (408, 409),\n",
       "   (409, 410),\n",
       "   (410, 411),\n",
       "   (411, 412),\n",
       "   (412, 413),\n",
       "   (413, 414),\n",
       "   (414, 415),\n",
       "   (415, 416),\n",
       "   (416, 417),\n",
       "   (417, 418),\n",
       "   (418, 419),\n",
       "   (419, 420),\n",
       "   (420, 421),\n",
       "   (421, 422),\n",
       "   (422, 424),\n",
       "   (424, 425),\n",
       "   (425, 426),\n",
       "   (426, 427),\n",
       "   (427, 428),\n",
       "   (428, 429),\n",
       "   (429, 430),\n",
       "   (430, 431),\n",
       "   (431, 432),\n",
       "   (432, 433),\n",
       "   (433, 434),\n",
       "   (434, 435),\n",
       "   (435, 436),\n",
       "   (436, 437),\n",
       "   (437, 438),\n",
       "   (438, 439),\n",
       "   (439, 440),\n",
       "   (440, 441),\n",
       "   (441, 442),\n",
       "   (442, 443),\n",
       "   (443, 444),\n",
       "   (444, 445),\n",
       "   (445, 446),\n",
       "   (446, 447),\n",
       "   (447, 448),\n",
       "   (448, 449),\n",
       "   (449, 450),\n",
       "   (450, 451),\n",
       "   (451, 452),\n",
       "   (452, 453),\n",
       "   (453, 454),\n",
       "   (454, 455),\n",
       "   (455, 456),\n",
       "   (456, 457),\n",
       "   (457, 458),\n",
       "   (458, 459),\n",
       "   (459, 460),\n",
       "   (460, 461),\n",
       "   (461, 462),\n",
       "   (462, 463),\n",
       "   (463, 464),\n",
       "   (464, 465),\n",
       "   (465, 466),\n",
       "   (466, 467),\n",
       "   (467, 468),\n",
       "   (468, 469),\n",
       "   (469, 470),\n",
       "   (470, 471),\n",
       "   (471, 472),\n",
       "   (472, 473),\n",
       "   (473, 474),\n",
       "   (474, 475),\n",
       "   (475, 476),\n",
       "   (476, 477),\n",
       "   (477, 478),\n",
       "   (478, 479),\n",
       "   (479, 480),\n",
       "   (480, 481),\n",
       "   (481, 482),\n",
       "   (482, 483),\n",
       "   (483, 484),\n",
       "   (484, 485),\n",
       "   (485, 486),\n",
       "   (486, 487),\n",
       "   (487, 488),\n",
       "   (488, 489),\n",
       "   (489, 490),\n",
       "   (490, 491),\n",
       "   (491, 492),\n",
       "   (492, 493),\n",
       "   (493, 494),\n",
       "   (494, 495),\n",
       "   (495, 499),\n",
       "   (499, 500),\n",
       "   (500, 501),\n",
       "   (501, 502),\n",
       "   (502, 503),\n",
       "   (503, 504),\n",
       "   (504, 505),\n",
       "   (505, 506),\n",
       "   (506, 507),\n",
       "   (507, 508),\n",
       "   (508, 509),\n",
       "   (509, 510),\n",
       "   (510, 511),\n",
       "   (511, 512),\n",
       "   (512, 513),\n",
       "   (513, 514),\n",
       "   (514, 516),\n",
       "   (516, 517),\n",
       "   (517, 518),\n",
       "   (518, 519),\n",
       "   (519, 520),\n",
       "   (520, 521),\n",
       "   (521, 522),\n",
       "   (522, 523),\n",
       "   (523, 524),\n",
       "   (524, 525),\n",
       "   (525, 526),\n",
       "   (526, 527),\n",
       "   (527, 528),\n",
       "   (528, 529),\n",
       "   (529, 530),\n",
       "   (530, 531),\n",
       "   (531, 532),\n",
       "   (532, 533),\n",
       "   (533, 534),\n",
       "   (534, 535),\n",
       "   (535, 536),\n",
       "   (0, 0)]],\n",
       " 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_mapping, len(offset_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['1963年'], 'answer_start': [30]} 30 35 17 510 47 48\n",
      "token answer decode: 1963 年\n",
      "{'text': ['1990年被擢升为天主教河内总教区宗座署理'], 'answer_start': [41]} 41 62 15 510 53 70\n",
      "token answer decode: 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理\n"
     ]
    }
   ],
   "source": [
    "# 处理截断的过程中，答案被截断的情况\n",
    "\n",
    "for idx, offset in enumerate(offset_mapping):\n",
    "    answer = sample_dataset[idx]['answers']\n",
    "    start_char = answer['answer_start'][0]\n",
    "    end_char = start_char + len(answer['text'][0])\n",
    "\n",
    "    # 定位答案再token中的起始位置和结束位置\n",
    "    # 一种策略，我们要拿到context的起始和终止位置，然后左右逼近\n",
    "    # sequence_ids(idx) 方法返回一个列表，用于指示第 idx 个示例的每个分词后的 token 属于哪个输入序列。\n",
    "\n",
    "    context_start = tokenizer_examples.sequence_ids(idx).index(1)\n",
    "    context_end = tokenizer_examples.sequence_ids(idx).index(None, context_start) - 1 # 从context_start开始查找第一个出现的None\n",
    "\n",
    "    # 判断答案不在上下文中\n",
    "    if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "        start_token_pos = 0\n",
    "        end_token_pos = 0\n",
    "    else:\n",
    "        # 找到包含答案起始位置的上下文的开头\n",
    "        token_id = context_start\n",
    "        while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "            token_id += 1\n",
    "        start_token_pos = token_id\n",
    "\n",
    "        # 找到包含答案终止位置的上下文的结束\n",
    "        token_id = context_end\n",
    "        while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "            token_id -= 1\n",
    "        end_token_pos = token_id\n",
    "    \n",
    "    print(answer, start_char, end_char, context_start, context_end, start_token_pos, end_token_pos)\n",
    "    print(\"token answer decode:\", tokenizer.decode(tokenizer_examples['input_ids'][idx][start_token_pos : end_token_pos + 1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(examples):\n",
    "    tokenizer_examples = tokenizer(text=examples['question'],\n",
    "                                    text_pair=examples['context'],\n",
    "                                    max_length=512,\n",
    "                                    truncation='only_second',\n",
    "                                    return_offsets_mapping=True,\n",
    "                                    padding='max_length'\n",
    "                                   )\n",
    "    offset_mapping = tokenizer_examples.pop('offset_mapping')\n",
    "\n",
    "    # 因为后面会使用到tokenizer batch的方式进行处理\n",
    "    start_position = []\n",
    "    end_position = []\n",
    "    for idx, offset in enumerate(offset_mapping):\n",
    "        answer = examples['answers'][idx]\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = start_char + len(answer['text'][0])\n",
    "\n",
    "        # 定位答案再token中的起始位置和结束位置\n",
    "        # 一种策略，我们要拿到context的起始和终止位置，然后左右逼近\n",
    "        # sequence_ids(idx) 方法返回一个列表，用于指示第 idx 个示例的每个分词后的 token 属于哪个输入序列。\n",
    "\n",
    "        context_start = tokenizer_examples.sequence_ids(idx).index(1)\n",
    "        context_end = tokenizer_examples.sequence_ids(idx).index(None, context_start) - 1 # 从context_start开始查找第一个出现的None\n",
    "\n",
    "        # 判断答案不在上下文中\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else:\n",
    "            # 找到包含答案起始位置的上下文的开头\n",
    "            token_id = context_start\n",
    "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "                token_id += 1\n",
    "            start_token_pos = token_id\n",
    "\n",
    "            # 找到包含答案终止位置的上下文的结束\n",
    "            token_id = context_end\n",
    "            while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "                token_id -= 1\n",
    "            end_token_pos = token_id\n",
    "        start_position.append(start_token_pos)\n",
    "        end_position.append(end_token_pos)\n",
    "    \n",
    "    tokenizer_examples['start_positions'] = start_position\n",
    "    tokenizer_examples['end_positions'] = end_position\n",
    "    return tokenizer_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'answers'],\n",
       "    num_rows: 10142\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 10142\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 3219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_datasets = datasets.map(function=process_function, batched=True, remove_columns=datasets['train'].column_names)\n",
    "tokenizer_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at D:/pretrained_model/models--hfl--chinese-macbert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at D:/pretrained_model/models--hfl--chinese-macbert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 12%|█▏        | 100/805 [00:33<03:36,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0902, 'learning_rate': 4.3788819875776396e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 200/805 [01:06<03:15,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.704, 'learning_rate': 3.7577639751552796e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 300/805 [01:38<02:52,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4841, 'learning_rate': 3.136645962732919e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 400/805 [02:10<02:08,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3686, 'learning_rate': 2.515527950310559e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 500/805 [02:41<01:32,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1522, 'learning_rate': 1.894409937888199e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 600/805 [03:12<01:02,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1744, 'learning_rate': 1.2732919254658385e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 700/805 [03:43<00:32,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0959, 'learning_rate': 6.521739130434783e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 800/805 [04:13<00:01,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9661, 'learning_rate': 3.1055900621118013e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 805/805 [04:15<00:00,  3.59it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 23\u001b[0m\n\u001b[0;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models_for_qa\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     16\u001b[0m     args \u001b[38;5;241m=\u001b[39m args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDefaultDataCollator()\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1642\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1644\u001b[0m )\n\u001b[1;32m-> 1645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\trainer.py:2026\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2026\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   2030\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\trainer.py:2312\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   2311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2312\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2315\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\trainer.py:3043\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3040\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3042\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3043\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3044\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3046\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3047\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3053\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\trainer.py:3235\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3232\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   3234\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 3235\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3236\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\trainer.py:3455\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   3452\u001b[0m     return_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_return_loss\n\u001b[0;32m   3453\u001b[0m loss_without_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_names) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m return_loss \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3455\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\trainer.py:2688\u001b[0m, in \u001b[0;36mTrainer._prepare_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, Any]]:\n\u001b[0;32m   2684\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m \u001b[38;5;124;03m    Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\u001b[39;00m\n\u001b[0;32m   2686\u001b[0m \u001b[38;5;124;03m    handling potential state.\u001b[39;00m\n\u001b[0;32m   2687\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2688\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2690\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2691\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch received was empty, your model won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be able to train on it. Double-check that your \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2692\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining dataset contains keys expected by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2693\u001b[0m         )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\trainer.py:2670\u001b[0m, in \u001b[0;36mTrainer._prepare_input\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2666\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2667\u001b[0m \u001b[38;5;124;03mPrepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\u001b[39;00m\n\u001b[0;32m   2668\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m-> 2670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)({k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m   2672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\trainer.py:2670\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2666\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2667\u001b[0m \u001b[38;5;124;03mPrepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\u001b[39;00m\n\u001b[0;32m   2668\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m-> 2670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)({k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m   2672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\trainer.py:2680\u001b[0m, in \u001b[0;36mTrainer._prepare_input\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mis_floating_point(data) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(data)):\n\u001b[0;32m   2676\u001b[0m         \u001b[38;5;66;03m# NLP models inputs are int/uint and those get adjusted to the right dtype of the\u001b[39;00m\n\u001b[0;32m   2677\u001b[0m         \u001b[38;5;66;03m# embedding. Other models such as wav2vec2's inputs are already float and thus\u001b[39;00m\n\u001b[0;32m   2678\u001b[0m         \u001b[38;5;66;03m# may need special handling to match the dtypes of the model\u001b[39;00m\n\u001b[0;32m   2679\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdeepspeed_plugin\u001b[38;5;241m.\u001b[39mhf_ds_config\u001b[38;5;241m.\u001b[39mdtype()})\n\u001b[1;32m-> 2680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('D:/pretrained_model/models--hfl--chinese-macbert-base')\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./models_for_qa',\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=4,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args = args,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset= tokenizer_datasets['validation'],\n",
    "    eval_dataset= tokenizer_datasets['test'],\n",
    "    data_collator=DefaultDataCollator()\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step9 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\py39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion-answering\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m pipe(question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m小明在哪里上班？\u001b[39m\u001b[38;5;124m\"\u001b[39m, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m小明在北京上班。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "pipe(question=\"小明在哪里上班？\", context=\"小明在北京上班。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
