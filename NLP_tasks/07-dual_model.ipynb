{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本相似度实例（单模型单塔模型）模型分别编码两句话再进行分类-交互式"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset('json', data_files='./data/train_pair_1w.json', split=\"train\") # 如果是加载固定的json文件则用load_dataset\n",
    "# dataset = DatasetDict.load_from_disk('./data') # 加载的是huggingface的数据集\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = dataset.train_test_split(test_size=0.2)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': '我们也是沿着不变的轴心旋转，像地球一样，远看是星球，近看是泥土，而且有白日和黑夜交替着出现吗？',\n",
       " 'sentence2': '人也跟地球一样有南极和北极吗？',\n",
       " 'label': '1'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a8439320d94fb09370383f71286c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7976f731fb884cdd9201b461677dfa51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/pretrained_model/models--hfl--chinese-macbert-base\")\n",
    "\n",
    "def process_function(examples):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sen1, sen2, label in zip(examples[\"sentence1\"], examples[\"sentence2\"], examples[\"label\"]):\n",
    "        sentences.append(sen1)\n",
    "        sentences.append(sen2)\n",
    "        labels.append(1 if int(label) == 1 else 0)\n",
    "    \n",
    "    tokenizer_examples = tokenizer(sentences, max_length=250, truncation=True, padding=\"max_length\")\n",
    "    tokenizer_examples = {k: [v[i : i + 2] for i in range(0, len(v), 2)] for k, v in tokenizer_examples.items()}\n",
    "    tokenizer_examples['labels'] = labels\n",
    "    return tokenizer_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets['train'].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49207\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertForSequenceClassification, BertPreTrainedModel, BertModel\n",
    "from typing import Optional\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from torch.nn import CosineSimilarity, CosineEmbeddingLoss\n",
    "\n",
    "class DualModel(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.bert = BertModel(config)\n",
    "        self.post_init()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        senA_input_ids, senB_input_ids = input_ids[:, 0], input_ids[:, 1]\n",
    "        senA_attention_mask, senB_attention_mask = attention_mask[:, 0], attention_mask[:, 1]\n",
    "        senA_token_type_ids, senB_token_type_ids = token_type_ids[:, 0], token_type_ids[:, 1]\n",
    "\n",
    "        senA_outputs = self.bert(\n",
    "            senA_input_ids,\n",
    "            attention_mask=senA_attention_mask,\n",
    "            token_type_ids=senA_token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "        senA_pooled_output = senA_outputs[1]\n",
    "\n",
    "        senB_outputs = self.bert(\n",
    "            senB_input_ids,\n",
    "            attention_mask=senB_attention_mask,\n",
    "            token_type_ids=senB_token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "        senB_pooled_output = senB_outputs[1]\n",
    "\n",
    "        cos = CosineSimilarity()(senA_pooled_output, senB_pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CosineEmbeddingLoss(0.3)\n",
    "            loss = loss_fct(senA_pooled_output, senB_pooled_output, labels)\n",
    "        output = (cos,)\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "model = DualModel.from_pretrained('D:/pretrained_model/models--hfl--chinese-macbert-base')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "acc_metric = evaluate.load(\"./metric_accuracy.py\")\n",
    "f1_metirc = evaluate.load(\"./metric_f1.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(eval_predict):\n",
    "    predictions, labels = eval_predict\n",
    "    predictions = [int(p > 0.5) for p in predictions]\n",
    "    labels = [int(l) for l in labels]\n",
    "    # predictions = predictions.argmax(axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metirc.compute(predictions=predictions, references=labels)\n",
    "    acc.update(f1)\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 创建TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(output_dir=\"./dual_model\",      # 输出文件夹\n",
    "                               per_device_train_batch_size=4,  # 训练时的batch_size\n",
    "                               per_device_eval_batch_size=32,   # 验证时的batch_size\n",
    "                               logging_steps=10,                # log 打印的频率\n",
    "                               evaluation_strategy=\"epoch\",           # 评估策略\n",
    "                               save_strategy=\"epoch\",           # 保存策略\n",
    "                               save_total_limit=3,              # 最大保存数\n",
    "                               learning_rate=2e-6,              # 学习率\n",
    "                               weight_decay=0.005,               # weight_decay\n",
    "                               metric_for_best_model=\"f1\",      # 设定评估指标\n",
    "                               load_best_model_at_end=True,\n",
    "                               num_train_epochs=1\n",
    "                            #    max_steps=1000\n",
    "                               )     # 训练完成后加载最优模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 创建Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49207\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, \n",
    "                  args=train_args, \n",
    "                  tokenizer=tokenizer,\n",
    "                  train_dataset=tokenized_datasets[\"train\"], \n",
    "                  eval_dataset=tokenized_datasets[\"test\"], \n",
    "                  compute_metrics=eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a3a2e1970c4de786d2b6df295bacea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0172, 'grad_norm': 0.7525479197502136, 'learning_rate': 1.99e-06, 'epoch': 0.01}\n",
      "{'loss': 0.017, 'grad_norm': 0.34213656187057495, 'learning_rate': 1.98e-06, 'epoch': 0.01}\n",
      "{'loss': 0.011, 'grad_norm': 0.5554874539375305, 'learning_rate': 1.9699999999999998e-06, 'epoch': 0.01}\n",
      "{'loss': 0.0095, 'grad_norm': 0.7245813608169556, 'learning_rate': 1.96e-06, 'epoch': 0.02}\n",
      "{'loss': 0.0119, 'grad_norm': 0.2016068696975708, 'learning_rate': 1.95e-06, 'epoch': 0.03}\n",
      "{'loss': 0.0107, 'grad_norm': 1.620959997177124, 'learning_rate': 1.94e-06, 'epoch': 0.03}\n",
      "{'loss': 0.0148, 'grad_norm': 0.5462867617607117, 'learning_rate': 1.9299999999999997e-06, 'epoch': 0.04}\n",
      "{'loss': 0.005, 'grad_norm': 0.32554593682289124, 'learning_rate': 1.92e-06, 'epoch': 0.04}\n",
      "{'loss': 0.005, 'grad_norm': 0.3355433940887451, 'learning_rate': 1.91e-06, 'epoch': 0.04}\n",
      "{'loss': 0.0073, 'grad_norm': 0.2069815844297409, 'learning_rate': 1.8999999999999998e-06, 'epoch': 0.05}\n",
      "{'loss': 0.0071, 'grad_norm': 0.4088366627693176, 'learning_rate': 1.89e-06, 'epoch': 0.06}\n",
      "{'loss': 0.0067, 'grad_norm': 0.2179005742073059, 'learning_rate': 1.8799999999999998e-06, 'epoch': 0.06}\n",
      "{'loss': 0.0064, 'grad_norm': 0.308422327041626, 'learning_rate': 1.87e-06, 'epoch': 0.07}\n",
      "{'loss': 0.0061, 'grad_norm': 0.33983302116394043, 'learning_rate': 1.86e-06, 'epoch': 0.07}\n",
      "{'loss': 0.006, 'grad_norm': 0.24284714460372925, 'learning_rate': 1.85e-06, 'epoch': 0.07}\n",
      "{'loss': 0.0042, 'grad_norm': 0.11790302395820618, 'learning_rate': 1.84e-06, 'epoch': 0.08}\n",
      "{'loss': 0.0028, 'grad_norm': 0.08052590489387512, 'learning_rate': 1.83e-06, 'epoch': 0.09}\n",
      "{'loss': 0.0042, 'grad_norm': 0.0, 'learning_rate': 1.82e-06, 'epoch': 0.09}\n",
      "{'loss': 0.0025, 'grad_norm': 0.0, 'learning_rate': 1.81e-06, 'epoch': 0.1}\n",
      "{'loss': 0.0044, 'grad_norm': 0.0, 'learning_rate': 1.8e-06, 'epoch': 0.1}\n",
      "{'loss': 0.0038, 'grad_norm': 0.12143110483884811, 'learning_rate': 1.79e-06, 'epoch': 0.1}\n",
      "{'loss': 0.0033, 'grad_norm': 0.3190062642097473, 'learning_rate': 1.78e-06, 'epoch': 0.11}\n",
      "{'loss': 0.0026, 'grad_norm': 0.11559120565652847, 'learning_rate': 1.77e-06, 'epoch': 0.12}\n",
      "{'loss': 0.003, 'grad_norm': 0.1380114108324051, 'learning_rate': 1.7599999999999999e-06, 'epoch': 0.12}\n",
      "{'loss': 0.0039, 'grad_norm': 0.17355850338935852, 'learning_rate': 1.75e-06, 'epoch': 0.12}\n",
      "{'loss': 0.0019, 'grad_norm': 0.14939379692077637, 'learning_rate': 1.7399999999999999e-06, 'epoch': 0.13}\n",
      "{'loss': 0.0047, 'grad_norm': 0.19321243464946747, 'learning_rate': 1.73e-06, 'epoch': 0.14}\n",
      "{'loss': 0.0029, 'grad_norm': 0.0, 'learning_rate': 1.7199999999999998e-06, 'epoch': 0.14}\n",
      "{'loss': 0.0026, 'grad_norm': 0.08301163464784622, 'learning_rate': 1.71e-06, 'epoch': 0.14}\n",
      "{'loss': 0.0027, 'grad_norm': 0.043314434587955475, 'learning_rate': 1.6999999999999998e-06, 'epoch': 0.15}\n",
      "{'loss': 0.0023, 'grad_norm': 0.0, 'learning_rate': 1.69e-06, 'epoch': 0.15}\n",
      "{'loss': 0.0032, 'grad_norm': 0.14591942727565765, 'learning_rate': 1.6799999999999998e-06, 'epoch': 0.16}\n",
      "{'loss': 0.0017, 'grad_norm': 0.0, 'learning_rate': 1.6699999999999999e-06, 'epoch': 0.17}\n",
      "{'loss': 0.003, 'grad_norm': 0.10224353522062302, 'learning_rate': 1.6599999999999998e-06, 'epoch': 0.17}\n",
      "{'loss': 0.0033, 'grad_norm': 0.11658422648906708, 'learning_rate': 1.6499999999999999e-06, 'epoch': 0.17}\n",
      "{'loss': 0.0023, 'grad_norm': 0.039184048771858215, 'learning_rate': 1.6399999999999998e-06, 'epoch': 0.18}\n",
      "{'loss': 0.0017, 'grad_norm': 0.0, 'learning_rate': 1.6299999999999999e-06, 'epoch': 0.18}\n",
      "{'loss': 0.0024, 'grad_norm': 0.08394965529441833, 'learning_rate': 1.62e-06, 'epoch': 0.19}\n",
      "{'loss': 0.002, 'grad_norm': 0.02774255909025669, 'learning_rate': 1.61e-06, 'epoch': 0.2}\n",
      "{'loss': 0.0021, 'grad_norm': 0.17386573553085327, 'learning_rate': 1.6e-06, 'epoch': 0.2}\n",
      "{'loss': 0.0026, 'grad_norm': 0.12450628727674484, 'learning_rate': 1.59e-06, 'epoch': 0.2}\n",
      "{'loss': 0.0027, 'grad_norm': 0.0494757816195488, 'learning_rate': 1.58e-06, 'epoch': 0.21}\n",
      "{'loss': 0.0024, 'grad_norm': 0.05152355134487152, 'learning_rate': 1.57e-06, 'epoch': 0.21}\n",
      "{'loss': 0.0017, 'grad_norm': 0.02989261783659458, 'learning_rate': 1.5599999999999999e-06, 'epoch': 0.22}\n",
      "{'loss': 0.0014, 'grad_norm': 0.0, 'learning_rate': 1.55e-06, 'epoch': 0.23}\n",
      "{'loss': 0.0014, 'grad_norm': 0.0, 'learning_rate': 1.5399999999999999e-06, 'epoch': 0.23}\n",
      "{'loss': 0.0019, 'grad_norm': 0.03427847474813461, 'learning_rate': 1.53e-06, 'epoch': 0.23}\n",
      "{'loss': 0.0012, 'grad_norm': 0.031829796731472015, 'learning_rate': 1.5199999999999998e-06, 'epoch': 0.24}\n",
      "{'loss': 0.0019, 'grad_norm': 0.10558170080184937, 'learning_rate': 1.51e-06, 'epoch': 0.24}\n",
      "{'loss': 0.0019, 'grad_norm': 0.022414248436689377, 'learning_rate': 1.5e-06, 'epoch': 0.25}\n",
      "{'loss': 0.0014, 'grad_norm': 0.05914021283388138, 'learning_rate': 1.49e-06, 'epoch': 0.26}\n",
      "{'loss': 0.0017, 'grad_norm': 0.05594002455472946, 'learning_rate': 1.48e-06, 'epoch': 0.26}\n",
      "{'loss': 0.0016, 'grad_norm': 0.0, 'learning_rate': 1.47e-06, 'epoch': 0.27}\n",
      "{'loss': 0.0019, 'grad_norm': 0.04375762864947319, 'learning_rate': 1.46e-06, 'epoch': 0.27}\n",
      "{'loss': 0.0015, 'grad_norm': 0.052432071417570114, 'learning_rate': 1.4499999999999999e-06, 'epoch': 0.28}\n",
      "{'loss': 0.0016, 'grad_norm': 0.07183960825204849, 'learning_rate': 1.44e-06, 'epoch': 0.28}\n",
      "{'loss': 0.0013, 'grad_norm': 0.05220453441143036, 'learning_rate': 1.4299999999999999e-06, 'epoch': 0.28}\n",
      "{'loss': 0.001, 'grad_norm': 0.03202596306800842, 'learning_rate': 1.42e-06, 'epoch': 0.29}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0, 'learning_rate': 1.4099999999999998e-06, 'epoch': 0.29}\n",
      "{'loss': 0.0011, 'grad_norm': 0.0, 'learning_rate': 1.4e-06, 'epoch': 0.3}\n",
      "{'loss': 0.002, 'grad_norm': 0.0, 'learning_rate': 1.3899999999999998e-06, 'epoch': 0.3}\n",
      "{'loss': 0.0019, 'grad_norm': 0.09174279123544693, 'learning_rate': 1.38e-06, 'epoch': 0.31}\n",
      "{'loss': 0.0011, 'grad_norm': 0.0651661604642868, 'learning_rate': 1.37e-06, 'epoch': 0.32}\n",
      "{'loss': 0.0012, 'grad_norm': 0.03912809118628502, 'learning_rate': 1.3600000000000001e-06, 'epoch': 0.32}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0, 'learning_rate': 1.35e-06, 'epoch': 0.33}\n",
      "{'loss': 0.0017, 'grad_norm': 0.07761335372924805, 'learning_rate': 1.34e-06, 'epoch': 0.33}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0310693196952343, 'learning_rate': 1.33e-06, 'epoch': 0.34}\n",
      "{'loss': 0.0011, 'grad_norm': 0.04156101495027542, 'learning_rate': 1.32e-06, 'epoch': 0.34}\n",
      "{'loss': 0.0016, 'grad_norm': 0.0282848309725523, 'learning_rate': 1.31e-06, 'epoch': 0.34}\n",
      "{'loss': 0.0014, 'grad_norm': 0.0, 'learning_rate': 1.3e-06, 'epoch': 0.35}\n",
      "{'loss': 0.0018, 'grad_norm': 0.08636321127414703, 'learning_rate': 1.29e-06, 'epoch': 0.35}\n",
      "{'loss': 0.0011, 'grad_norm': 0.0, 'learning_rate': 1.28e-06, 'epoch': 0.36}\n",
      "{'loss': 0.0011, 'grad_norm': 0.021087810397148132, 'learning_rate': 1.27e-06, 'epoch': 0.36}\n",
      "{'loss': 0.0012, 'grad_norm': 0.025976793840527534, 'learning_rate': 1.26e-06, 'epoch': 0.37}\n",
      "{'loss': 0.0012, 'grad_norm': 0.022981341928243637, 'learning_rate': 1.2499999999999999e-06, 'epoch': 0.38}\n",
      "{'loss': 0.0013, 'grad_norm': 0.026484087109565735, 'learning_rate': 1.24e-06, 'epoch': 0.38}\n",
      "{'loss': 0.0016, 'grad_norm': 0.08258049935102463, 'learning_rate': 1.2299999999999999e-06, 'epoch': 0.39}\n",
      "{'loss': 0.0013, 'grad_norm': 0.04962523654103279, 'learning_rate': 1.22e-06, 'epoch': 0.39}\n",
      "{'loss': 0.0012, 'grad_norm': 0.04439569637179375, 'learning_rate': 1.2099999999999998e-06, 'epoch': 0.4}\n",
      "{'loss': 0.0009, 'grad_norm': 0.013724619522690773, 'learning_rate': 1.2e-06, 'epoch': 0.4}\n",
      "{'loss': 0.0013, 'grad_norm': 0.05026871711015701, 'learning_rate': 1.1899999999999998e-06, 'epoch': 0.41}\n",
      "{'loss': 0.0013, 'grad_norm': 0.0, 'learning_rate': 1.18e-06, 'epoch': 0.41}\n",
      "{'loss': 0.0015, 'grad_norm': 0.07172311097383499, 'learning_rate': 1.1699999999999998e-06, 'epoch': 0.41}\n",
      "{'loss': 0.0013, 'grad_norm': 0.037526682019233704, 'learning_rate': 1.16e-06, 'epoch': 0.42}\n",
      "{'loss': 0.0013, 'grad_norm': 0.022303368896245956, 'learning_rate': 1.1499999999999998e-06, 'epoch': 0.42}\n",
      "{'loss': 0.0009, 'grad_norm': 0.041947804391384125, 'learning_rate': 1.1399999999999999e-06, 'epoch': 0.43}\n",
      "{'loss': 0.0015, 'grad_norm': 0.022882433608174324, 'learning_rate': 1.1299999999999998e-06, 'epoch': 0.43}\n",
      "{'loss': 0.0009, 'grad_norm': 0.04712790995836258, 'learning_rate': 1.12e-06, 'epoch': 0.44}\n",
      "{'loss': 0.001, 'grad_norm': 0.03672386333346367, 'learning_rate': 1.11e-06, 'epoch': 0.45}\n",
      "{'loss': 0.0014, 'grad_norm': 0.024991651996970177, 'learning_rate': 1.1e-06, 'epoch': 0.45}\n",
      "{'loss': 0.0005, 'grad_norm': 0.01767008751630783, 'learning_rate': 1.09e-06, 'epoch': 0.46}\n",
      "{'loss': 0.001, 'grad_norm': 0.0229966938495636, 'learning_rate': 1.08e-06, 'epoch': 0.46}\n",
      "{'loss': 0.0013, 'grad_norm': 0.041506025940179825, 'learning_rate': 1.07e-06, 'epoch': 0.47}\n",
      "{'loss': 0.0011, 'grad_norm': 0.0681736022233963, 'learning_rate': 1.06e-06, 'epoch': 0.47}\n",
      "{'loss': 0.0012, 'grad_norm': 0.01917234994471073, 'learning_rate': 1.05e-06, 'epoch': 0.47}\n",
      "{'loss': 0.0008, 'grad_norm': 0.06148756295442581, 'learning_rate': 1.04e-06, 'epoch': 0.48}\n",
      "{'loss': 0.0011, 'grad_norm': 0.02816028520464897, 'learning_rate': 1.0299999999999999e-06, 'epoch': 0.48}\n",
      "{'loss': 0.0012, 'grad_norm': 0.05063045024871826, 'learning_rate': 1.02e-06, 'epoch': 0.49}\n",
      "{'loss': 0.0006, 'grad_norm': 0.06013805791735649, 'learning_rate': 1.0099999999999999e-06, 'epoch': 0.49}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0, 'learning_rate': 1e-06, 'epoch': 0.5}\n",
      "{'loss': 0.001, 'grad_norm': 0.04718029871582985, 'learning_rate': 9.9e-07, 'epoch': 0.51}\n",
      "{'loss': 0.0005, 'grad_norm': 0.0, 'learning_rate': 9.8e-07, 'epoch': 0.51}\n",
      "{'loss': 0.0015, 'grad_norm': 0.05849757790565491, 'learning_rate': 9.7e-07, 'epoch': 0.52}\n",
      "{'loss': 0.0007, 'grad_norm': 0.031187117099761963, 'learning_rate': 9.6e-07, 'epoch': 0.52}\n",
      "{'loss': 0.0009, 'grad_norm': 0.045226871967315674, 'learning_rate': 9.499999999999999e-07, 'epoch': 0.53}\n",
      "{'loss': 0.0007, 'grad_norm': 0.034253280609846115, 'learning_rate': 9.399999999999999e-07, 'epoch': 0.53}\n",
      "{'loss': 0.0009, 'grad_norm': 0.054538726806640625, 'learning_rate': 9.3e-07, 'epoch': 0.54}\n",
      "{'loss': 0.0012, 'grad_norm': 0.05104321613907814, 'learning_rate': 9.2e-07, 'epoch': 0.54}\n",
      "{'loss': 0.0007, 'grad_norm': 0.05217069759964943, 'learning_rate': 9.1e-07, 'epoch': 0.55}\n",
      "{'loss': 0.0009, 'grad_norm': 0.044074758887290955, 'learning_rate': 9e-07, 'epoch': 0.55}\n",
      "{'loss': 0.0011, 'grad_norm': 0.043740320950746536, 'learning_rate': 8.9e-07, 'epoch': 0.56}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0, 'learning_rate': 8.799999999999999e-07, 'epoch': 0.56}\n",
      "{'loss': 0.0006, 'grad_norm': 0.02774748206138611, 'learning_rate': 8.699999999999999e-07, 'epoch': 0.56}\n",
      "{'loss': 0.0007, 'grad_norm': 0.04129234701395035, 'learning_rate': 8.599999999999999e-07, 'epoch': 0.57}\n",
      "{'loss': 0.001, 'grad_norm': 0.0410035140812397, 'learning_rate': 8.499999999999999e-07, 'epoch': 0.57}\n",
      "{'loss': 0.0009, 'grad_norm': 0.027715502306818962, 'learning_rate': 8.399999999999999e-07, 'epoch': 0.58}\n",
      "{'loss': 0.001, 'grad_norm': 0.023284848779439926, 'learning_rate': 8.299999999999999e-07, 'epoch': 0.58}\n",
      "{'loss': 0.0008, 'grad_norm': 0.02108049765229225, 'learning_rate': 8.199999999999999e-07, 'epoch': 0.59}\n",
      "{'loss': 0.0009, 'grad_norm': 0.035815171897411346, 'learning_rate': 8.1e-07, 'epoch': 0.59}\n",
      "{'loss': 0.0012, 'grad_norm': 0.03755044937133789, 'learning_rate': 8e-07, 'epoch': 0.6}\n",
      "{'loss': 0.0009, 'grad_norm': 0.013392933644354343, 'learning_rate': 7.9e-07, 'epoch': 0.6}\n",
      "{'loss': 0.0007, 'grad_norm': 0.02031511627137661, 'learning_rate': 7.799999999999999e-07, 'epoch': 0.61}\n",
      "{'loss': 0.0011, 'grad_norm': 0.016815312206745148, 'learning_rate': 7.699999999999999e-07, 'epoch': 0.61}\n",
      "{'loss': 0.0008, 'grad_norm': 0.016995007172226906, 'learning_rate': 7.599999999999999e-07, 'epoch': 0.62}\n",
      "{'loss': 0.0008, 'grad_norm': 0.04149777814745903, 'learning_rate': 7.5e-07, 'epoch': 0.62}\n",
      "{'loss': 0.0006, 'grad_norm': 0.025247523561120033, 'learning_rate': 7.4e-07, 'epoch': 0.63}\n",
      "{'loss': 0.0005, 'grad_norm': 0.01777772232890129, 'learning_rate': 7.3e-07, 'epoch': 0.64}\n",
      "{'loss': 0.001, 'grad_norm': 0.011974957771599293, 'learning_rate': 7.2e-07, 'epoch': 0.64}\n",
      "{'loss': 0.0008, 'grad_norm': 0.03998949006199837, 'learning_rate': 7.1e-07, 'epoch': 0.65}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0159981120377779, 'learning_rate': 7e-07, 'epoch': 0.65}\n",
      "{'loss': 0.0006, 'grad_norm': 0.05453348532319069, 'learning_rate': 6.9e-07, 'epoch': 0.66}\n",
      "{'loss': 0.0007, 'grad_norm': 0.04609314352273941, 'learning_rate': 6.800000000000001e-07, 'epoch': 0.66}\n",
      "{'loss': 0.0005, 'grad_norm': 0.0, 'learning_rate': 6.7e-07, 'epoch': 0.67}\n",
      "{'loss': 0.0006, 'grad_norm': 0.019702114164829254, 'learning_rate': 6.6e-07, 'epoch': 0.67}\n",
      "{'loss': 0.0007, 'grad_norm': 0.02975574880838394, 'learning_rate': 6.5e-07, 'epoch': 0.68}\n",
      "{'loss': 0.0005, 'grad_norm': 0.015732211992144585, 'learning_rate': 6.4e-07, 'epoch': 0.68}\n",
      "{'loss': 0.0005, 'grad_norm': 0.0, 'learning_rate': 6.3e-07, 'epoch': 0.69}\n",
      "{'loss': 0.0009, 'grad_norm': 0.042174000293016434, 'learning_rate': 6.2e-07, 'epoch': 0.69}\n",
      "{'loss': 0.0011, 'grad_norm': 0.026022864505648613, 'learning_rate': 6.1e-07, 'epoch': 0.69}\n",
      "{'loss': 0.0005, 'grad_norm': 0.04249335452914238, 'learning_rate': 6e-07, 'epoch': 0.7}\n",
      "{'loss': 0.0007, 'grad_norm': 0.023989863693714142, 'learning_rate': 5.9e-07, 'epoch': 0.7}\n",
      "{'loss': 0.0007, 'grad_norm': 0.028811892494559288, 'learning_rate': 5.8e-07, 'epoch': 0.71}\n",
      "{'loss': 0.0007, 'grad_norm': 0.026802152395248413, 'learning_rate': 5.699999999999999e-07, 'epoch': 0.71}\n",
      "{'loss': 0.0008, 'grad_norm': 0.028786515817046165, 'learning_rate': 5.6e-07, 'epoch': 0.72}\n",
      "{'loss': 0.0006, 'grad_norm': 0.028301728889346123, 'learning_rate': 5.5e-07, 'epoch': 0.72}\n",
      "{'loss': 0.0009, 'grad_norm': 0.03966842591762543, 'learning_rate': 5.4e-07, 'epoch': 0.73}\n",
      "{'loss': 0.0009, 'grad_norm': 0.036358460783958435, 'learning_rate': 5.3e-07, 'epoch': 0.73}\n",
      "{'loss': 0.0007, 'grad_norm': 0.015254625119268894, 'learning_rate': 5.2e-07, 'epoch': 0.74}\n",
      "{'loss': 0.0005, 'grad_norm': 0.010200846940279007, 'learning_rate': 5.1e-07, 'epoch': 0.74}\n",
      "{'loss': 0.0006, 'grad_norm': 0.03166675940155983, 'learning_rate': 5e-07, 'epoch': 0.75}\n",
      "{'loss': 0.0007, 'grad_norm': 0.06351657211780548, 'learning_rate': 4.9e-07, 'epoch': 0.76}\n",
      "{'loss': 0.0004, 'grad_norm': 0.018706249073147774, 'learning_rate': 4.8e-07, 'epoch': 0.76}\n",
      "{'loss': 0.0004, 'grad_norm': 0.008364361710846424, 'learning_rate': 4.6999999999999995e-07, 'epoch': 0.77}\n",
      "{'loss': 0.0007, 'grad_norm': 0.05241457000374794, 'learning_rate': 4.6e-07, 'epoch': 0.77}\n",
      "{'loss': 0.0007, 'grad_norm': 0.01517460960894823, 'learning_rate': 4.5e-07, 'epoch': 0.78}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0, 'learning_rate': 4.3999999999999997e-07, 'epoch': 0.78}\n",
      "{'loss': 0.0008, 'grad_norm': 0.0452188141644001, 'learning_rate': 4.2999999999999996e-07, 'epoch': 0.79}\n",
      "{'loss': 0.0005, 'grad_norm': 0.0, 'learning_rate': 4.1999999999999995e-07, 'epoch': 0.79}\n",
      "{'loss': 0.0007, 'grad_norm': 0.020568925887346268, 'learning_rate': 4.0999999999999994e-07, 'epoch': 0.8}\n",
      "{'loss': 0.0005, 'grad_norm': 0.011549009941518307, 'learning_rate': 4e-07, 'epoch': 0.8}\n",
      "{'loss': 0.0006, 'grad_norm': 0.03144853189587593, 'learning_rate': 3.8999999999999997e-07, 'epoch': 0.81}\n",
      "{'loss': 0.0008, 'grad_norm': 0.011481690220534801, 'learning_rate': 3.7999999999999996e-07, 'epoch': 0.81}\n",
      "{'loss': 0.0005, 'grad_norm': 0.04161324352025986, 'learning_rate': 3.7e-07, 'epoch': 0.81}\n",
      "{'loss': 0.0005, 'grad_norm': 0.017787085846066475, 'learning_rate': 3.6e-07, 'epoch': 0.82}\n",
      "{'loss': 0.0006, 'grad_norm': 0.026166513562202454, 'learning_rate': 3.5e-07, 'epoch': 0.82}\n",
      "{'loss': 0.0009, 'grad_norm': 0.03451237082481384, 'learning_rate': 3.4000000000000003e-07, 'epoch': 0.83}\n",
      "{'loss': 0.0007, 'grad_norm': 0.02165561355650425, 'learning_rate': 3.3e-07, 'epoch': 0.83}\n",
      "{'loss': 0.0008, 'grad_norm': 0.03182660788297653, 'learning_rate': 3.2e-07, 'epoch': 0.84}\n",
      "{'loss': 0.0007, 'grad_norm': 0.029946520924568176, 'learning_rate': 3.1e-07, 'epoch': 0.84}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0, 'learning_rate': 3e-07, 'epoch': 0.85}\n",
      "{'loss': 0.0006, 'grad_norm': 0.011490195989608765, 'learning_rate': 2.9e-07, 'epoch': 0.85}\n",
      "{'loss': 0.0009, 'grad_norm': 0.03886561840772629, 'learning_rate': 2.8e-07, 'epoch': 0.86}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0, 'learning_rate': 2.7e-07, 'epoch': 0.86}\n",
      "{'loss': 0.0006, 'grad_norm': 0.03159480541944504, 'learning_rate': 2.6e-07, 'epoch': 0.87}\n",
      "{'loss': 0.0007, 'grad_norm': 0.02153908647596836, 'learning_rate': 2.5e-07, 'epoch': 0.88}\n",
      "{'loss': 0.0005, 'grad_norm': 0.007621096912771463, 'learning_rate': 2.4e-07, 'epoch': 0.88}\n",
      "{'loss': 0.0007, 'grad_norm': 0.029577072709798813, 'learning_rate': 2.3e-07, 'epoch': 0.89}\n",
      "{'loss': 0.0005, 'grad_norm': 0.03280167654156685, 'learning_rate': 2.1999999999999998e-07, 'epoch': 0.89}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0, 'learning_rate': 2.0999999999999997e-07, 'epoch': 0.9}\n",
      "{'loss': 0.0005, 'grad_norm': 0.02159835956990719, 'learning_rate': 2e-07, 'epoch': 0.9}\n",
      "{'loss': 0.0007, 'grad_norm': 0.025874748826026917, 'learning_rate': 1.8999999999999998e-07, 'epoch': 0.91}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0, 'learning_rate': 1.8e-07, 'epoch': 0.91}\n",
      "{'loss': 0.0006, 'grad_norm': 0.015164469368755817, 'learning_rate': 1.7000000000000001e-07, 'epoch': 0.92}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0, 'learning_rate': 1.6e-07, 'epoch': 0.92}\n",
      "{'loss': 0.0007, 'grad_norm': 0.019466428086161613, 'learning_rate': 1.5e-07, 'epoch': 0.93}\n",
      "{'loss': 0.0006, 'grad_norm': 0.04284067451953888, 'learning_rate': 1.4e-07, 'epoch': 0.93}\n",
      "{'loss': 0.0007, 'grad_norm': 0.02365587092936039, 'learning_rate': 1.3e-07, 'epoch': 0.94}\n",
      "{'loss': 0.0006, 'grad_norm': 0.011873840354382992, 'learning_rate': 1.2e-07, 'epoch': 0.94}\n",
      "{'loss': 0.0007, 'grad_norm': 0.012297668494284153, 'learning_rate': 1.0999999999999999e-07, 'epoch': 0.94}\n",
      "{'loss': 0.0007, 'grad_norm': 0.027723105624318123, 'learning_rate': 1e-07, 'epoch': 0.95}\n",
      "{'loss': 0.0005, 'grad_norm': 0.020264023914933205, 'learning_rate': 9e-08, 'epoch': 0.95}\n",
      "{'loss': 0.0006, 'grad_norm': 0.02628258615732193, 'learning_rate': 8e-08, 'epoch': 0.96}\n",
      "{'loss': 0.0008, 'grad_norm': 0.02377353608608246, 'learning_rate': 7e-08, 'epoch': 0.96}\n",
      "{'loss': 0.0007, 'grad_norm': 0.02301802672445774, 'learning_rate': 6e-08, 'epoch': 0.97}\n",
      "{'loss': 0.0006, 'grad_norm': 0.007765205577015877, 'learning_rate': 5e-08, 'epoch': 0.97}\n",
      "{'loss': 0.0009, 'grad_norm': 0.03185306489467621, 'learning_rate': 4e-08, 'epoch': 0.98}\n",
      "{'loss': 0.0004, 'grad_norm': 0.02320314757525921, 'learning_rate': 3e-08, 'epoch': 0.98}\n",
      "{'loss': 0.0006, 'grad_norm': 0.038434870541095734, 'learning_rate': 2e-08, 'epoch': 0.99}\n",
      "{'loss': 0.0006, 'grad_norm': 0.02090536430478096, 'learning_rate': 1e-08, 'epoch': 0.99}\n",
      "{'loss': 0.0006, 'grad_norm': 0.033002614974975586, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4f0cfeca18458580cfe105559a4a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.252792369807139e-05, 'eval_accuracy': 0.397, 'eval_f1': 0.5683607730851825, 'eval_runtime': 37.8537, 'eval_samples_per_second': 52.835, 'eval_steps_per_second': 1.664, 'epoch': 1.0}\n",
      "{'train_runtime': 493.0964, 'train_samples_per_second': 16.224, 'train_steps_per_second': 4.056, 'train_loss': 0.0018450733870267867, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.0018450733870267867, metrics={'train_runtime': 493.0964, 'train_samples_per_second': 16.224, 'train_steps_per_second': 4.056, 'train_loss': 0.0018450733870267867, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step10 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step11 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceSimilarityPipeline:\n",
    "\n",
    "    def __init__(self, model, tokenizer) -> None:\n",
    "        self.model = model.bert\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "\n",
    "    def preprocess(self, senA, senB):\n",
    "        return self.tokenizer([senA, senB], max_length=128, truncation=True, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        return self.model(**inputs)[1]  # [2, 768]\n",
    "\n",
    "    def postprocess(self, logits):\n",
    "        cos = CosineSimilarity()(logits[None, 0, :], logits[None,1, :]).squeeze().cpu().item()\n",
    "        return cos\n",
    "\n",
    "    def __call__(self, senA, senB, return_vector=False):\n",
    "        inputs = self.preprocess(senA, senB)\n",
    "        logits = self.predict(inputs)\n",
    "        result = self.postprocess(logits)\n",
    "        if return_vector:\n",
    "            return result, logits\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = SentenceSimilarityPipeline(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"我喜欢北京\", \"明天不行\", return_vector=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_langchainchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
