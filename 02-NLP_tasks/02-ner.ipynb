{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Transformers的NER实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "import evaluate # evaluate=0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2 加载数据集（加载数据集，查看数据特征）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49207\\AppData\\Roaming\\Python\\Python311\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for peoples_daily_ner contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/peoples_daily_ner\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_datasets = load_dataset('peoples_daily_ner', cache_dir='./data')\n",
    "ner_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_datasets.keys()\n",
    "print(ner_datasets['train'][0]) # 查看数据\n",
    "ner_datasets['train'].features # 查看数据的特征\n",
    "\n",
    "label_list = ner_datasets['train'].features[\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建tokenizer,并进行datasets的构建，进行dataloader构建\n",
    "tokenizer = AutoTokenizer.from_pretrained('D:/AI/pretrain_model/models--hfl--chinese-macbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(ner_datasets['train'][0]['tokens'], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3862, 6501, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = tokenizer(\"海豹比赛地点在厦门与金门之间的海域\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.word_ids(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(ner_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 借助word_ids构建NER的标签\n",
    "def process_function(examples):\n",
    "    # truncation 允许截断，这里的截断是按照一定的规则进行截断，而不是直接截断\n",
    "    tokenized_examples = tokenizer(examples['tokens'], max_length=128, truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_examples.word_ids(i)\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                # word_ids是词的下标index，label是ner_tags中的标签，就是取对应token的标签\n",
    "                label_ids.append(label[word_id])\n",
    "        labels.append(label_ids)\n",
    "    tokenized_examples['labels'] = labels\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceaa1a766c26495a93c5974b8a8db191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffda675ac2a441e98297167a8866d7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995a5a6ff28e4fdcbfdbac653cc10434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4637 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = ner_datasets.map(process_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at D:/AI/pretrain_model/models--hfl--chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('D:/AI/pretrain_model/models--hfl--chinese-macbert-base', num_labels=len(label_list)) # 模型默认是二分类，需要用num_labels指定分类的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step5 创建评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"seqeval\", module_type: \"metric\", features: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')}, usage: \"\"\"\n",
       "Produces labelling scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n",
       "    references: List of List of reference labels (Ground truth (correct) target values)\n",
       "    suffix: True if the IOB prefix is after type, False otherwise. default: False\n",
       "    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n",
       "        default: None\n",
       "    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n",
       "        If you want to only count exact matches, pass mode=\"strict\". default: None.\n",
       "    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n",
       "    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n",
       "        \"warn\". \"warn\" acts as 0, but the warning is raised.\n",
       "\n",
       "Returns:\n",
       "    'scores': dict. Summary of the scores for overall and per type\n",
       "        Overall:\n",
       "            'accuracy': accuracy,\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure,\n",
       "        Per type:\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> seqeval = evaluate.load(\"seqeval\")\n",
       "    >>> results = seqeval.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n",
       "    >>> print(results[\"overall_f1\"])\n",
       "    0.5\n",
       "    >>> print(results[\"PER\"][\"f1\"])\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqeval = evaluate.load(\"seqeval_metric.py\")\n",
    "seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_metric(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # 将id转换为原始的字符串类型的标签\n",
    "    true_predictions = [\n",
    "        [label_list[p] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels) \n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels) \n",
    "    ]\n",
    "\n",
    "    result = seqeval.compute(predictions=true_predictions, references=true_labels, mode=\"strict\", scheme=\"IOB2\")\n",
    "\n",
    "    return {\n",
    "        \"f1\": result[\"overall_f1\"]\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step6 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='model_for_ner',\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=128,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model='f1',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step7 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['validation'],\n",
    "    compute_metrics=eval_metric,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step8 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2752d609bdf549bbaa8b3ee937c73e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0201, 'grad_norm': 0.04536016285419464, 'learning_rate': 4.952079739313782e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0224, 'grad_norm': 0.08559268712997437, 'learning_rate': 4.904159478627564e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0295, 'grad_norm': 0.010819493792951107, 'learning_rate': 4.8562392179413456e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0197, 'grad_norm': 4.265042781829834, 'learning_rate': 4.808318957255128e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0156, 'grad_norm': 0.798666775226593, 'learning_rate': 4.76039869656891e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0125, 'grad_norm': 0.00754921417683363, 'learning_rate': 4.712478435882691e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0143, 'grad_norm': 2.220876693725586, 'learning_rate': 4.6645581751964735e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0123, 'grad_norm': 1.3952579498291016, 'learning_rate': 4.616637914510255e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0162, 'grad_norm': 3.518247127532959, 'learning_rate': 4.568717653824037e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0216, 'grad_norm': 1.6854336261749268, 'learning_rate': 4.520797393137819e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0081, 'grad_norm': 0.005621638614684343, 'learning_rate': 4.4728771324516007e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0099, 'grad_norm': 0.43619999289512634, 'learning_rate': 4.4249568717653824e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0265, 'grad_norm': 0.12111496925354004, 'learning_rate': 4.377036611079164e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0251, 'grad_norm': 0.015498846769332886, 'learning_rate': 4.329116350392947e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0113, 'grad_norm': 0.0027185524813830853, 'learning_rate': 4.2811960897067285e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0094, 'grad_norm': 0.011521335691213608, 'learning_rate': 4.2332758290205096e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0193, 'grad_norm': 0.003427663119509816, 'learning_rate': 4.185355568334292e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0217, 'grad_norm': 1.6363791227340698, 'learning_rate': 4.137435307648074e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0099, 'grad_norm': 0.0056137265637516975, 'learning_rate': 4.089515046961856e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0203, 'grad_norm': 0.01393173635005951, 'learning_rate': 4.0415947862756375e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0429, 'grad_norm': 0.043848007917404175, 'learning_rate': 3.993674525589419e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0275, 'grad_norm': 4.412378787994385, 'learning_rate': 3.945754264903202e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0177, 'grad_norm': 0.07877393066883087, 'learning_rate': 3.897834004216983e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0334, 'grad_norm': 0.0219917930662632, 'learning_rate': 3.8499137435307647e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0173, 'grad_norm': 2.385732412338257, 'learning_rate': 3.801993482844547e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0218, 'grad_norm': 0.008027482777833939, 'learning_rate': 3.754073222158328e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0177, 'grad_norm': 0.16166798770427704, 'learning_rate': 3.706152961472111e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0264, 'grad_norm': 0.49102407693862915, 'learning_rate': 3.6582327007858925e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0089, 'grad_norm': 0.24229250848293304, 'learning_rate': 3.610312440099674e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0253, 'grad_norm': 0.19295582175254822, 'learning_rate': 3.562392179413456e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0199, 'grad_norm': 0.09108515828847885, 'learning_rate': 3.514471918727238e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0212, 'grad_norm': 0.07959331572055817, 'learning_rate': 3.4665516580410204e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0225, 'grad_norm': 0.16095538437366486, 'learning_rate': 3.4186313973548015e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0318, 'grad_norm': 0.002862280933186412, 'learning_rate': 3.370711136668583e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0086, 'grad_norm': 0.1666283756494522, 'learning_rate': 3.322790875982366e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0301, 'grad_norm': 0.002836445113644004, 'learning_rate': 3.2748706152961475e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0262, 'grad_norm': 0.10620211809873581, 'learning_rate': 3.226950354609929e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0245, 'grad_norm': 0.017915979027748108, 'learning_rate': 3.179030093923711e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0169, 'grad_norm': 0.02420904114842415, 'learning_rate': 3.131109833237493e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0132, 'grad_norm': 0.9439449906349182, 'learning_rate': 3.083189572551275e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0375, 'grad_norm': 0.011912055313587189, 'learning_rate': 3.0352693118650565e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0173, 'grad_norm': 3.883805513381958, 'learning_rate': 2.9873490511788386e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0127, 'grad_norm': 0.007796416059136391, 'learning_rate': 2.9394287904926204e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0276, 'grad_norm': 0.023147670552134514, 'learning_rate': 2.8915085298064022e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0179, 'grad_norm': 0.5636870861053467, 'learning_rate': 2.8435882691201844e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0178, 'grad_norm': 2.6183876991271973, 'learning_rate': 2.7956680084339658e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0219, 'grad_norm': 1.1978187561035156, 'learning_rate': 2.7477477477477483e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0122, 'grad_norm': 2.971301317214966, 'learning_rate': 2.6998274870615297e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0111, 'grad_norm': 0.0365813784301281, 'learning_rate': 2.6519072263753115e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0098, 'grad_norm': 0.0014713889686390758, 'learning_rate': 2.6039869656890937e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0121, 'grad_norm': 0.009613274596631527, 'learning_rate': 2.556066705002875e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0137, 'grad_norm': 0.7127346396446228, 'learning_rate': 2.5081464443166576e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0202, 'grad_norm': 0.6290315985679626, 'learning_rate': 2.460226183630439e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0092, 'grad_norm': 0.010198998264968395, 'learning_rate': 2.4123059229442212e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0114, 'grad_norm': 0.03578690439462662, 'learning_rate': 2.3643856622580026e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0086, 'grad_norm': 0.01162114180624485, 'learning_rate': 2.3164654015717844e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0143, 'grad_norm': 1.576243281364441, 'learning_rate': 2.2685451408855666e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0162, 'grad_norm': 0.005513428244739771, 'learning_rate': 2.2206248801993484e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0231, 'grad_norm': 0.012222941033542156, 'learning_rate': 2.1727046195131305e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0105, 'grad_norm': 0.022024570032954216, 'learning_rate': 2.124784358826912e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0303, 'grad_norm': 1.690077781677246, 'learning_rate': 2.076864098140694e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0181, 'grad_norm': 0.0015400669071823359, 'learning_rate': 2.028943837454476e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0094, 'grad_norm': 0.001154808676801622, 'learning_rate': 1.9810235767682577e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0108, 'grad_norm': 0.9998900890350342, 'learning_rate': 1.9331033160820398e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0173, 'grad_norm': 0.0033649676479399204, 'learning_rate': 1.8851830553958213e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0258, 'grad_norm': 0.005040342919528484, 'learning_rate': 1.8372627947096034e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0115, 'grad_norm': 0.1938236802816391, 'learning_rate': 1.7893425340233852e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0083, 'grad_norm': 0.002994466805830598, 'learning_rate': 1.741422273337167e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0049, 'grad_norm': 0.05540074408054352, 'learning_rate': 1.6935020126509488e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0081, 'grad_norm': 0.001516435295343399, 'learning_rate': 1.6455817519647306e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0344, 'grad_norm': 2.1517062187194824, 'learning_rate': 1.5976614912785127e-05, 'epoch': 0.68}\n",
      "{'loss': 0.021, 'grad_norm': 0.12329678982496262, 'learning_rate': 1.5497412305922945e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0155, 'grad_norm': 0.010352801531553268, 'learning_rate': 1.5018209699060765e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0158, 'grad_norm': 0.04732886701822281, 'learning_rate': 1.4539007092198581e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0077, 'grad_norm': 0.006559770554304123, 'learning_rate': 1.40598044853364e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0103, 'grad_norm': 0.0013278117403388023, 'learning_rate': 1.358060187847422e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0133, 'grad_norm': 0.009089392609894276, 'learning_rate': 1.3101399271612038e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0137, 'grad_norm': 5.48840856552124, 'learning_rate': 1.2622196664749858e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0097, 'grad_norm': 0.9802805781364441, 'learning_rate': 1.2142994057887676e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0204, 'grad_norm': 0.8516781330108643, 'learning_rate': 1.1663791451025494e-05, 'epoch': 0.77}\n",
      "{'loss': 0.005, 'grad_norm': 0.592771053314209, 'learning_rate': 1.1184588844163313e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0231, 'grad_norm': 0.039534226059913635, 'learning_rate': 1.0705386237301133e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0077, 'grad_norm': 0.00768081983551383, 'learning_rate': 1.0226183630438949e-05, 'epoch': 0.8}\n",
      "{'loss': 0.004, 'grad_norm': 0.004540756810456514, 'learning_rate': 9.746981023576769e-06, 'epoch': 0.81}\n",
      "{'loss': 0.0095, 'grad_norm': 9.986960411071777, 'learning_rate': 9.267778416714587e-06, 'epoch': 0.81}\n",
      "{'loss': 0.0112, 'grad_norm': 0.01898999512195587, 'learning_rate': 8.788575809852406e-06, 'epoch': 0.82}\n",
      "{'loss': 0.0118, 'grad_norm': 6.850070953369141, 'learning_rate': 8.309373202990224e-06, 'epoch': 0.83}\n",
      "{'loss': 0.0075, 'grad_norm': 0.5732966065406799, 'learning_rate': 7.830170596128044e-06, 'epoch': 0.84}\n",
      "{'loss': 0.0061, 'grad_norm': 0.002726183505728841, 'learning_rate': 7.350967989265863e-06, 'epoch': 0.85}\n",
      "{'loss': 0.0091, 'grad_norm': 0.025394342839717865, 'learning_rate': 6.871765382403681e-06, 'epoch': 0.86}\n",
      "{'loss': 0.0178, 'grad_norm': 0.0013580078957602382, 'learning_rate': 6.3925627755414994e-06, 'epoch': 0.87}\n",
      "{'loss': 0.0108, 'grad_norm': 0.0007405724609270692, 'learning_rate': 5.913360168679318e-06, 'epoch': 0.88}\n",
      "{'loss': 0.0133, 'grad_norm': 0.017645563930273056, 'learning_rate': 5.434157561817137e-06, 'epoch': 0.89}\n",
      "{'loss': 0.0119, 'grad_norm': 0.02386499010026455, 'learning_rate': 4.954954954954955e-06, 'epoch': 0.9}\n",
      "{'loss': 0.0099, 'grad_norm': 1.4768059253692627, 'learning_rate': 4.475752348092774e-06, 'epoch': 0.91}\n",
      "{'loss': 0.0062, 'grad_norm': 0.004508711397647858, 'learning_rate': 3.9965497412305925e-06, 'epoch': 0.92}\n",
      "{'loss': 0.0094, 'grad_norm': 0.004084781743586063, 'learning_rate': 3.517347134368411e-06, 'epoch': 0.93}\n",
      "{'loss': 0.0124, 'grad_norm': 0.0310463048517704, 'learning_rate': 3.0381445275062297e-06, 'epoch': 0.94}\n",
      "{'loss': 0.0119, 'grad_norm': 0.04886915534734726, 'learning_rate': 2.558941920644048e-06, 'epoch': 0.95}\n",
      "{'loss': 0.0168, 'grad_norm': 0.02556501142680645, 'learning_rate': 2.0797393137818673e-06, 'epoch': 0.96}\n",
      "{'loss': 0.0184, 'grad_norm': 0.6776461005210876, 'learning_rate': 1.6005367069196856e-06, 'epoch': 0.97}\n",
      "{'loss': 0.0187, 'grad_norm': 0.9830728769302368, 'learning_rate': 1.1213341000575044e-06, 'epoch': 0.98}\n",
      "{'loss': 0.004, 'grad_norm': 0.002568584866821766, 'learning_rate': 6.42131493195323e-07, 'epoch': 0.99}\n",
      "{'loss': 0.0044, 'grad_norm': 4.600165843963623, 'learning_rate': 1.6292888633314166e-07, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66f5fe2a872451f8c3d7b8aad9ec00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02287866175174713, 'eval_f1': 0.951490999038065, 'eval_runtime': 10.9232, 'eval_samples_per_second': 212.301, 'eval_steps_per_second': 1.739, 'epoch': 1.0}\n",
      "{'train_runtime': 438.6879, 'train_samples_per_second': 47.562, 'train_steps_per_second': 11.892, 'train_loss': 0.01623696371276008, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5217, training_loss=0.01623696371276008, metrics={'train_runtime': 438.6879, 'train_samples_per_second': 47.562, 'train_steps_per_second': 11.892, 'train_loss': 0.01623696371276008, 'epoch': 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step9 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"D:/AI/pretrain_model/models--hfl--chinese-macbert-base\",\n",
       "  \"architectures\": [\n",
       "    \"BertForTokenClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.38.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"D:/AI/pretrain_model/models--hfl--chinese-macbert-base\",\n",
       "  \"architectures\": [\n",
       "    \"BertForTokenClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-PER\",\n",
       "    \"2\": \"I-PER\",\n",
       "    \"3\": \"B-ORG\",\n",
       "    \"4\": \"I-ORG\",\n",
       "    \"5\": \"B-LOC\",\n",
       "    \"6\": \"I-LOC\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.38.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label = {idx:label  for idx, label in enumerate(label_list)}\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(task='token-classification', model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-LOC', 'score': 0.9991673, 'index': 3, 'word': '深', 'start': 2, 'end': 3}, {'entity': 'I-LOC', 'score': 0.9994618, 'index': 4, 'word': '圳', 'start': 3, 'end': 4}, {'entity': 'B-LOC', 'score': 0.9979067, 'index': 5, 'word': '福', 'start': 4, 'end': 5}, {'entity': 'I-LOC', 'score': 0.9992625, 'index': 6, 'word': '田', 'start': 5, 'end': 6}, {'entity': 'B-LOC', 'score': 0.99845827, 'index': 14, 'word': '车', 'start': 13, 'end': 14}, {'entity': 'I-LOC', 'score': 0.9983924, 'index': 15, 'word': '公', 'start': 14, 'end': 15}, {'entity': 'I-LOC', 'score': 0.9985978, 'index': 16, 'word': '庙', 'start': 15, 'end': 16}]\n"
     ]
    }
   ],
   "source": [
    "res = ner_pipeline(\"我在深圳福田上班，我老婆在车公庙上班\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9992747,\n",
       "  'word': '小 明',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99945104,\n",
       "  'word': '北 京',\n",
       "  'start': 3,\n",
       "  'end': 5}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipeline2 = pipeline(task='token-classification', model=model, tokenizer=tokenizer, device=0, aggregation_strategy='simple')\n",
    "res2 = ner_pipeline2(\"小明在北京上班\")\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PER': ['小明'], 'LOC': ['北京']}\n"
     ]
    }
   ],
   "source": [
    "ner_res = {}\n",
    "intput_data = \"小明在北京上班\"\n",
    "for r in res2:\n",
    "    if r['entity_group'] not in ner_res:\n",
    "        ner_res[r['entity_group']] = []\n",
    "    ner_res[r['entity_group']].append(intput_data[r[\"start\"]:r[\"end\"]])\n",
    "\n",
    "print(ner_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_langchainchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
