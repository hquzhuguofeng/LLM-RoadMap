{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Transformers的阅读理解实现-滑窗处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset('cmrc2018', cache_dir='./data')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('D:/pretrained_model/models--hfl--chinese-macbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = datasets['train'].select(range(5))\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_examples = tokenizer(text=sample_dataset['question'],\n",
    "                               text_pair=sample_dataset['context'],\n",
    "                               max_length=512,\n",
    "                               truncation='only_second',\n",
    "                               return_offsets_mapping=True,\n",
    "                               return_overflowing_tokens = True,\n",
    "                               stride=128,\n",
    "                               padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_examples['overflow_to_sample_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_examples['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_examples['overflow_to_sample_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in tokenizer.batch_decode(tokenizer_examples['input_ids']):\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mapping = tokenizer_examples.pop(\"overflow_to_sample_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理截断的过程中，答案被截断的情况\n",
    "\n",
    "for idx, _ in enumerate(sample_mapping):\n",
    "    answer = sample_dataset['answers'][sample_mapping[idx]]\n",
    "    start_char = answer['answer_start'][0]\n",
    "    end_char = start_char + len(answer['text'][0])\n",
    "\n",
    "    # 定位答案再token中的起始位置和结束位置\n",
    "    # 一种策略，我们要拿到context的起始和终止位置，然后左右逼近\n",
    "    # sequence_ids(idx) 方法返回一个列表，用于指示第 idx 个示例的每个分词后的 token 属于哪个输入序列。\n",
    "\n",
    "    context_start = tokenizer_examples.sequence_ids(idx).index(1)\n",
    "    context_end = tokenizer_examples.sequence_ids(idx).index(None, context_start) - 1 # 从context_start开始查找第一个出现的None\n",
    "\n",
    "    offset = tokenizer_examples.get(\"offset_mapping\")[idx]\n",
    "\n",
    "    # 判断答案不在上下文中\n",
    "    if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "        start_token_pos = 0\n",
    "        end_token_pos = 0\n",
    "    else:\n",
    "        # 找到包含答案起始位置的上下文的开头\n",
    "        token_id = context_start\n",
    "        while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "            token_id += 1\n",
    "        start_token_pos = token_id\n",
    "\n",
    "        # 找到包含答案终止位置的上下文的结束\n",
    "        token_id = context_end\n",
    "        while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "            token_id -= 1\n",
    "        end_token_pos = token_id\n",
    "    \n",
    "    print(answer, start_char, end_char, context_start, context_end, start_token_pos, end_token_pos)\n",
    "    print(\"token answer decode:\", tokenizer.decode(tokenizer_examples['input_ids'][idx][start_token_pos : end_token_pos + 1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(examples):\n",
    "    tokenizer_examples = tokenizer(text=examples['question'],\n",
    "                                    text_pair=examples['context'],\n",
    "                                    return_offsets_mapping=True,\n",
    "                                    return_overflowing_tokens = True,\n",
    "                                    stride=128,\n",
    "                                    max_length=512,\n",
    "                                    truncation='only_second',\n",
    "                                    padding='max_length'\n",
    "                                   )\n",
    "    sample_mapping = tokenizer_examples.pop('overflow_to_sample_mapping')\n",
    "\n",
    "    # 因为后面会使用到tokenizer batch的方式进行处理\n",
    "    start_position = []\n",
    "    end_position = []\n",
    "    examples_ids = [] # 用于记录答案是在原来段落中的哪个段落的\n",
    "    for idx, _ in enumerate(sample_mapping):\n",
    "        answer = examples[\"answers\"][sample_mapping[idx]]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        # 定位答案再token中的起始位置和结束位置\n",
    "        # 一种策略，我们要拿到context的起始和终止位置，然后左右逼近\n",
    "        # sequence_ids(idx) 方法返回一个列表，用于指示第 idx 个示例的每个分词后的 token 属于哪个输入序列。[none,0000,none,11111] 类似这种数据\n",
    "\n",
    "        context_start = tokenizer_examples.sequence_ids(idx).index(1)\n",
    "        context_end = tokenizer_examples.sequence_ids(idx).index(None, context_start) - 1 # 从context_start开始查找第一个出现的None\n",
    "        offset = tokenizer_examples.get(\"offset_mapping\")[idx] # 拿到分词后的偏移量\n",
    "\n",
    "        # 判断答案不在上下文中\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else:\n",
    "            # 找到包含答案起始位置的上下文的开头\n",
    "            token_id = context_start\n",
    "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "                token_id += 1\n",
    "            start_token_pos = token_id\n",
    "\n",
    "            # 找到包含答案终止位置的上下文的结束\n",
    "            token_id = context_end\n",
    "            while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "                token_id -= 1\n",
    "            end_token_pos = token_id\n",
    "\n",
    "        start_position.append(start_token_pos)\n",
    "        end_position.append(end_token_pos)\n",
    "        examples_ids.append(examples['id'][sample_mapping[idx]])\n",
    "        ## 用于标记答案在cls token上的情况，对应的位置标记为None, 这里涉及到的是 cls+question+sep+context+sep 的句子\n",
    "        # 并将非句子部分的内容标记为None\n",
    "        tokenizer_examples['offset_mapping'][idx] = [\n",
    "            (o if tokenizer_examples.sequence_ids(idx)[k] == 1 else None)\n",
    "            for k, o in enumerate(tokenizer_examples['offset_mapping'][idx])\n",
    "        ]\n",
    "    \n",
    "    tokenizer_examples[\"example_ids\"] = examples_ids\n",
    "    tokenizer_examples['start_positions'] = start_position\n",
    "    tokenizer_examples['end_positions'] = end_position\n",
    "    return tokenizer_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_datasets = datasets.map(function=process_function, batched=True, remove_columns=datasets['train'].column_names)\n",
    "tokenizer_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def get_result(start_logits, end_logits, exmaples, features):\n",
    "\n",
    "    predictions = {}\n",
    "    references = {}\n",
    "\n",
    "    # example 和 feature的映射\n",
    "    example_to_feature = collections.defaultdict(list)\n",
    "    for idx, example_id in enumerate(features[\"example_ids\"]):\n",
    "        example_to_feature[example_id].append(idx)\n",
    "\n",
    "    # 最优答案候选\n",
    "    n_best = 20\n",
    "    # 最大答案长度\n",
    "    max_answer_length = 30\n",
    "\n",
    "    for example in exmaples:\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        for feature_idx in example_to_feature[example_id]:\n",
    "            start_logit = start_logits[feature_idx]\n",
    "            end_logit = end_logits[feature_idx]\n",
    "            offset = features[feature_idx][\"offset_mapping\"]\n",
    "            '''\n",
    "            对 start_logit 和 end_logit 数组进行 降序排序，以找出得分最高的起始位置和结束位置。\n",
    "            然后，通过选择 前 n_best 个候选位置 来生成可能的答案边界，这样可以获得多个候选答案，之后再根据得分筛选出最佳答案。\n",
    "            '''\n",
    "            start_indexes = np.argsort(start_logit)[::-1][:n_best].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[::-1][:n_best].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if offset[start_index] is None or offset[end_index] is None:\n",
    "                        continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    # offset的长度和分词后的长度是一致的，因此start_index&end_indexes其实是分词后的答案索引信息，通过索引信息还原原文\n",
    "                    answers.append({\n",
    "                        \"text\": context[offset[start_index][0]: offset[end_index][1]],\n",
    "                        \"score\": start_logit[start_index] + end_logit[end_index]\n",
    "                    })\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"score\"])\n",
    "            predictions[example_id] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predictions[example_id] = \"\"\n",
    "        references[example_id] = example[\"answers\"][\"text\"]\n",
    "\n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmrc_eval import evaluate_cmrc\n",
    "\n",
    "def metirc(pred):\n",
    "    start_logits, end_logits = pred[0]\n",
    "    if start_logits.shape[0] == len(tokenizer_datasets[\"validation\"]):\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"validation\"], tokenizer_datasets[\"validation\"])\n",
    "    else:\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"test\"], tokenizer_datasets[\"test\"])\n",
    "    return evaluate_cmrc(p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('D:/pretrained_model/models--hfl--chinese-macbert-base')\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./models_for_qa',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=100,\n",
    "    max_steps=800\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args = args,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset= tokenizer_datasets['train'],\n",
    "    eval_dataset= tokenizer_datasets['validation'],\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=metirc\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step9 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "pipe(question=\"小明在哪里上班？\", context=\"小明在上海工作过，现在在深圳做了。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
