{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.qwen2 import Qwen2Config, Qwen2ForCausalLM\n",
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2MLP\n",
    "from transformers.activations import ACT2FN\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "from liger_kernel.ops.utils import calculate_settings\n",
    "from liger_kernel.ops.utils import ensure_contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@triton.jit\n",
    "def silu(x):\n",
    "    return x * tl.sigmoid(x)\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n",
    "    program_id = tl.program_id(0).to(tl.int64)\n",
    "\n",
    "    # locate start index\n",
    "    a_ptr += program_id * stride\n",
    "    b_ptr += program_id * stride\n",
    "    c_ptr += program_id * stride\n",
    "\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = col_offsets < n_cols\n",
    "\n",
    "    # sigmoid requires type float32\n",
    "    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n",
    "    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n",
    "    c_row = silu(a_row) * b_row\n",
    "    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _swiglu_backward_kernel(dc_ptr, a_ptr, b_ptr, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n",
    "    program_id = tl.program_id(0).to(tl.int64)\n",
    "\n",
    "    # locate start index\n",
    "    dc_ptr += program_id * stride\n",
    "    a_ptr += program_id * stride\n",
    "    b_ptr += program_id * stride\n",
    "\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = col_offsets < n_cols\n",
    "\n",
    "    dc_row = tl.load(dc_ptr + col_offsets, mask=mask, other=0)\n",
    "    # sigmoid requires type float32\n",
    "    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n",
    "    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n",
    "\n",
    "    # recomputation to save memory\n",
    "    sig_a = tl.sigmoid(a_row)\n",
    "    silu_a = a_row * sig_a\n",
    "    db_row = dc_row * silu_a\n",
    "    da_row = dc_row * (silu_a * (1 - sig_a) + sig_a) * b_row\n",
    "\n",
    "    tl.store(a_ptr + col_offsets, da_row, mask=mask)\n",
    "    tl.store(b_ptr + col_offsets, db_row, mask=mask)\n",
    "\n",
    "\n",
    "def swiglu_forward(a, b):\n",
    "    ori_shape = a.shape\n",
    "\n",
    "    n_cols = ori_shape[-1]\n",
    "    a = a.view(-1, n_cols)\n",
    "    b = b.view(-1, n_cols)\n",
    "    c = torch.empty_like(a)\n",
    "    n_rows = a.shape[0]\n",
    "\n",
    "    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n",
    "\n",
    "    _swiglu_forward_kernel[(n_rows,)](\n",
    "        a,\n",
    "        b,\n",
    "        c,\n",
    "        c.stride(-2),\n",
    "        n_cols=n_cols,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "        num_warps=num_warps,\n",
    "    )\n",
    "    return a, b, c.view(*ori_shape)\n",
    "\n",
    "\n",
    "def swiglu_backward(a, b, dc):\n",
    "    ori_shape = dc.shape\n",
    "    n_cols = ori_shape[-1]\n",
    "    dc = dc.view(-1, n_cols)\n",
    "    n_rows = dc.shape[0]\n",
    "\n",
    "    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n",
    "\n",
    "    _swiglu_backward_kernel[(n_rows,)](\n",
    "        dc,\n",
    "        a,\n",
    "        b,\n",
    "        dc.stride(-2),\n",
    "        n_cols=n_cols,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "        num_warps=num_warps,\n",
    "    )\n",
    "    return a.view(*ori_shape), b.view(*ori_shape)\n",
    "\n",
    "\n",
    "class LigerSiLUMulFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    @ensure_contiguous\n",
    "    def forward(ctx, a, b):\n",
    "        a, b, c = swiglu_forward(a, b)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return c\n",
    "\n",
    "    @staticmethod\n",
    "    @ensure_contiguous\n",
    "    def backward(ctx, dc):\n",
    "        a, b = ctx.saved_tensors\n",
    "        a, b = swiglu_backward(a, b, dc)\n",
    "        return a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2MLP2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "\n",
    "class Qwen2MLP3(Qwen2MLP):\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"edit by guofeng\")\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "    \n",
    "    \n",
    "class Qwen2MLP4(Qwen2MLP):\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"edit by guofeng\")\n",
    "        tmp = LigerSiLUMulFunction.apply(self.gate_proj(x)),self.up_proj(x)\n",
    "        down_proj = self.down_proj(tmp)\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mlp(type : str) -> None:\n",
    "\n",
    "    from transformers.models.qwen2 import modeling_qwen2\n",
    "\n",
    "    if type == 'simple':\n",
    "        modeling_qwen2.Qwen2MLP = Qwen2MLP2\n",
    "\n",
    "    elif type == 'v2':\n",
    "        modeling_qwen2.Qwen2MLP = Qwen2MLP3\n",
    "\n",
    "    elif type == 'liger_kernel':\n",
    "        modeling_qwen2.Qwen2MLP = Qwen2MLP4\n",
    "\n",
    "apply_mlp('liger_kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'D:/pretrained_model/models--Qwen--Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map='cuda',\n",
    "    torch_dtype='auto'\n",
    "    )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"给我介绍下深圳未来10年的发展\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个国家领导人，站在全球的视野，给出你专业的看法\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoModel的lazy加载方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 方法一用transformers导入方式进行安装\n",
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2ForCausalLM, Qwen2ForQuestionAnswering\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "Qwen2ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法二用字符串定义模型名字，然后注册得到具体的包\n",
    "import importlib\n",
    "\n",
    "model_name = 'qwen2.modeling_qwen2'\n",
    "model_package = importlib.import_module(name=f\".{model_name}\", package='transformers.models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'transformers.models.qwen2.modeling_qwen2' from 'c:\\\\Users\\\\49207\\\\.conda\\\\envs\\\\py311_langchainchat\\\\Lib\\\\site-packages\\\\transformers\\\\models\\\\qwen2\\\\modeling_qwen2.py'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qwen2ForCausalLM_gf = getattr(model_package, 'Qwen2ForCausalLM')\n",
    "Qwen2ForCausalLM_gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2ForQuestionAnswering"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qwen2ForQuestionAnswering_gf = getattr(model_package, 'Qwen2ForQuestionAnswering')\n",
    "Qwen2ForQuestionAnswering_gf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoModelForCausalLM\n",
    "\n",
    "\n",
    "_BaseAutoModelClass\n",
    "```\n",
    " - elif type(config) in cls._model_mapping.keys():\n",
    "        model_class = _get_model_class(config, cls._model_mapping) # 获取模型具体的类，比如Qwen2ForCausalLM\n",
    "        return model_class.from_pretrained(\n",
    "            pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n",
    "        )\n",
    "```           \n",
    "\n",
    "MODEL_FOR_CAUSAL_LM_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_FOR_CAUSAL_LM_MAPPING_NAMES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, key):\n",
    "    if key in self._extra_content:\n",
    "        return self._extra_content[key]\n",
    "    model_type = self._reverse_config_mapping[key.__name__]\n",
    "    if model_type in self._model_mapping:\n",
    "        model_name = self._model_mapping[model_type]\n",
    "        return self._load_attr_from_module(model_type, model_name)\n",
    "\n",
    "    # Maybe there was several model types associated with this config.\n",
    "    model_types = [k for k, v in self._config_mapping.items() if v == key.__name__]\n",
    "    for mtype in model_types:\n",
    "        if mtype in self._model_mapping:\n",
    "            model_name = self._model_mapping[mtype]\n",
    "            return self._load_attr_from_module(mtype, model_name)\n",
    "    raise KeyError(key)\n",
    "\n",
    "def keys(self):\n",
    "    mapping_keys = [\n",
    "        self._load_attr_from_module(key, name)\n",
    "        for key, name in self._config_mapping.items()\n",
    "        if key in self._model_mapping.keys()\n",
    "    ]\n",
    "    return mapping_keys + list(self._extra_content.keys())\n",
    "\n",
    "def get(self, key, default):\n",
    "    try:\n",
    "        return self.__getitem__(key)\n",
    "    except KeyError:\n",
    "        return default\n",
    "\n",
    "def __bool__(self):\n",
    "    return bool(self.keys())\n",
    "\n",
    "def values(self):\n",
    "    mapping_values = [\n",
    "        self._load_attr_from_module(key, name)\n",
    "        for key, name in self._model_mapping.items()\n",
    "        if key in self._config_mapping.keys()\n",
    "    ]\n",
    "    return mapping_values + list(self._extra_content.values())\n",
    "\n",
    "def _load_attr_from_module(self, model_type, attr):\n",
    "    module_name = model_type_to_module_name(model_type)\n",
    "    if module_name not in self._modules:\n",
    "        self._modules[module_name] = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n",
    "    return getattribute_from_module(self._modules[module_name], attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':1, 'b':2}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['a']  # __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get('a') # get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', 'b'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.keys() # keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.values() # values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_langchainchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
