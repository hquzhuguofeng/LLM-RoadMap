{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.qwen2 import Qwen2Config, Qwen2ForCausalLM\n",
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2MLP\n",
    "from transformers.activations import ACT2FN\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "from liger_kernel.ops.utils import calculate_settings\n",
    "from liger_kernel.ops.utils import ensure_contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@triton.jit\n",
    "def silu(x):\n",
    "    return x * tl.sigmoid(x)\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n",
    "    program_id = tl.program_id(0).to(tl.int64)\n",
    "\n",
    "    # locate start index\n",
    "    a_ptr += program_id * stride\n",
    "    b_ptr += program_id * stride\n",
    "    c_ptr += program_id * stride\n",
    "\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = col_offsets < n_cols\n",
    "\n",
    "    # sigmoid requires type float32\n",
    "    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n",
    "    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n",
    "    c_row = silu(a_row) * b_row\n",
    "    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _swiglu_backward_kernel(dc_ptr, a_ptr, b_ptr, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n",
    "    program_id = tl.program_id(0).to(tl.int64)\n",
    "\n",
    "    # locate start index\n",
    "    dc_ptr += program_id * stride\n",
    "    a_ptr += program_id * stride\n",
    "    b_ptr += program_id * stride\n",
    "\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = col_offsets < n_cols\n",
    "\n",
    "    dc_row = tl.load(dc_ptr + col_offsets, mask=mask, other=0)\n",
    "    # sigmoid requires type float32\n",
    "    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n",
    "    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n",
    "\n",
    "    # recomputation to save memory\n",
    "    sig_a = tl.sigmoid(a_row)\n",
    "    silu_a = a_row * sig_a\n",
    "    db_row = dc_row * silu_a\n",
    "    da_row = dc_row * (silu_a * (1 - sig_a) + sig_a) * b_row\n",
    "\n",
    "    tl.store(a_ptr + col_offsets, da_row, mask=mask)\n",
    "    tl.store(b_ptr + col_offsets, db_row, mask=mask)\n",
    "\n",
    "\n",
    "def swiglu_forward(a, b):\n",
    "    ori_shape = a.shape\n",
    "\n",
    "    n_cols = ori_shape[-1]\n",
    "    a = a.view(-1, n_cols)\n",
    "    b = b.view(-1, n_cols)\n",
    "    c = torch.empty_like(a)\n",
    "    n_rows = a.shape[0]\n",
    "\n",
    "    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n",
    "\n",
    "    _swiglu_forward_kernel[(n_rows,)](\n",
    "        a,\n",
    "        b,\n",
    "        c,\n",
    "        c.stride(-2),\n",
    "        n_cols=n_cols,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "        num_warps=num_warps,\n",
    "    )\n",
    "    return a, b, c.view(*ori_shape)\n",
    "\n",
    "\n",
    "def swiglu_backward(a, b, dc):\n",
    "    ori_shape = dc.shape\n",
    "    n_cols = ori_shape[-1]\n",
    "    dc = dc.view(-1, n_cols)\n",
    "    n_rows = dc.shape[0]\n",
    "\n",
    "    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n",
    "\n",
    "    _swiglu_backward_kernel[(n_rows,)](\n",
    "        dc,\n",
    "        a,\n",
    "        b,\n",
    "        dc.stride(-2),\n",
    "        n_cols=n_cols,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "        num_warps=num_warps,\n",
    "    )\n",
    "    return a.view(*ori_shape), b.view(*ori_shape)\n",
    "\n",
    "\n",
    "class LigerSiLUMulFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    @ensure_contiguous\n",
    "    def forward(ctx, a, b):\n",
    "        a, b, c = swiglu_forward(a, b)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return c\n",
    "\n",
    "    @staticmethod\n",
    "    @ensure_contiguous\n",
    "    def backward(ctx, dc):\n",
    "        a, b = ctx.saved_tensors\n",
    "        a, b = swiglu_backward(a, b, dc)\n",
    "        return a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2MLP2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "\n",
    "class Qwen2MLP3(Qwen2MLP):\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"edit by guofeng\")\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "    \n",
    "    \n",
    "class Qwen2MLP4(Qwen2MLP):\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"edit by guofeng\")\n",
    "        tmp = LigerSiLUMulFunction.apply(self.gate_proj(x)),self.up_proj(x)\n",
    "        down_proj = self.down_proj(tmp)\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mlp(type : str) -> None:\n",
    "\n",
    "    from transformers.models.qwen2 import modeling_qwen2\n",
    "\n",
    "    if type == 'simple':\n",
    "        modeling_qwen2.Qwen2MLP = Qwen2MLP2\n",
    "\n",
    "    elif type == 'v2':\n",
    "        modeling_qwen2.Qwen2MLP = Qwen2MLP3\n",
    "\n",
    "    elif type == 'liger_kernel':\n",
    "        modeling_qwen2.Qwen2MLP = Qwen2MLP4\n",
    "\n",
    "apply_mlp('liger_kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'D:/pretrained_model/models--Qwen--Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map='cuda',\n",
    "    torch_dtype='auto'\n",
    "    )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"给我介绍下深圳未来10年的发展\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个国家领导人，站在全球的视野，给出你专业的看法\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
