{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama2 lora 16bit 实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(\"./data/alpaca_data_zh/\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。',\n",
       "  '4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。',\n",
       "  '朱利叶斯·凯撒，又称尤利乌斯·恺撒（Julius Caesar）是古罗马的政治家、军事家和作家。他于公元前44年3月15日被刺杀。 \\n\\n根据历史记载，当时罗马元老院里一些参议员联合起来策划了对恺撒的刺杀行动，因为他们担心恺撒的统治将给罗马共和制带来威胁。在公元前44年3月15日（又称“3月的艾达之日”），恺撒去参加元老院会议时，被一群参议员包围并被攻击致死。据记载，他身中23刀，其中一刀最终致命。'],\n",
       " 'input': ['', '输入：4/16', ''],\n",
       " 'instruction': ['保持健康的三个提示。', '解释为什么以下分数等同于1/4', '朱利叶斯·凯撒是如何死亡的？']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/pretrained_model/LLM-Research/Llama-3___2-1B\")\n",
    "tokenizer\n",
    "tokenizer.padding_side = \"right\"  # 一定要设置padding_side为right，否则batch大于1时可能不收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有bug\n",
    "def process_func_bak(example):\n",
    "    MAX_LENGTH = 1024\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 1024\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    response = tokenizer(example[\"output\"])\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 35075, 25, 111505, 69978, 113614, 9554, 126524, 46239, 3490, 72803, 25, 220, 128000, 88852, 21043, 118551, 113614, 9554, 126524, 46239, 49543, 16, 13, 111505, 69978, 111006, 108726, 1811, 74257, 36827, 102210, 108562, 40265, 9554, 111006, 114253, 116051, 107471, 65782, 5486, 110774, 65782, 58291, 83994, 126503, 3922, 27327, 113096, 42399, 64209, 104473, 36651, 113614, 3922, 50285, 103229, 120044, 107079, 120772, 91495, 19361, 103129, 35304, 111689, 83747, 33014, 30358, 3490, 17, 13, 111020, 229, 120383, 120522, 102456, 1811, 74257, 36827, 102456, 11883, 17039, 118882, 9554, 107139, 105, 108171, 5486, 53610, 28873, 5486, 37087, 104858, 53953, 34208, 121496, 57942, 103, 96412, 33857, 103167, 9554, 111678, 101828, 103706, 102456, 53953, 3922, 111098, 103048, 45736, 117587, 122603, 121496, 57942, 103, 34208, 117041, 119008, 105610, 118551, 113614, 9554, 120522, 102456, 105369, 33565, 107, 3490, 18, 13, 124022, 94, 120379, 105843, 102780, 1811, 113136, 120379, 33764, 17792, 33014, 113614, 57237, 30356, 107693, 3922, 13153, 8107, 17792, 74257, 36827, 51611, 123076, 220, 22, 12, 23, 103036, 13646, 9554, 113136, 120379, 1811, 103424, 110085, 113136, 120379, 19361, 103129, 35304, 111689, 106196, 106208, 48634, 3922, 113096, 42399, 111006, 123843, 59464, 91495, 115890, 61633, 48634, 34208, 41914, 124920, 48634, 1811, 128001]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Human: 朱利叶斯·凯撒是如何死亡的？\n",
      "\n",
      "Assistant: <|begin_of_text|>朱利叶斯·凯撒，又称尤利乌斯·恺撒（Julius Caesar）是古罗马的政治家、军事家和作家。他于公元前44年3月15日被刺杀。 \n",
      "\n",
      "根据历史记载，当时罗马元老院里一些参议员联合起来策划了对恺撒的刺杀行动，因为他们担心恺撒的统治将给罗马共和制带来威胁。在公元前44年3月15日（又称“3月的艾达之日”），恺撒去参加元老院会议时，被一群参议员包围并被攻击致死。据记载，他身中23刀，其中一刀最终致命。<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_ds[2][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(list(filter(lambda x: x != -100, tokenized_ds[1][\"labels\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "model = AutoModelForCausalLM.from_pretrained(\"D:/pretrained_model/LLM-Research/Llama-3___2-1B\", low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map='sequential')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.float16\n",
      "model.layers.0.self_attn.q_proj.weight torch.float16\n",
      "model.layers.0.self_attn.k_proj.weight torch.float16\n",
      "model.layers.0.self_attn.v_proj.weight torch.float16\n",
      "model.layers.0.self_attn.o_proj.weight torch.float16\n",
      "model.layers.0.mlp.gate_proj.weight torch.float16\n",
      "model.layers.0.mlp.up_proj.weight torch.float16\n",
      "model.layers.0.mlp.down_proj.weight torch.float16\n",
      "model.layers.0.input_layernorm.weight torch.float16\n",
      "model.layers.0.post_attention_layernorm.weight torch.float16\n",
      "model.layers.1.self_attn.q_proj.weight torch.float16\n",
      "model.layers.1.self_attn.k_proj.weight torch.float16\n",
      "model.layers.1.self_attn.v_proj.weight torch.float16\n",
      "model.layers.1.self_attn.o_proj.weight torch.float16\n",
      "model.layers.1.mlp.gate_proj.weight torch.float16\n",
      "model.layers.1.mlp.up_proj.weight torch.float16\n",
      "model.layers.1.mlp.down_proj.weight torch.float16\n",
      "model.layers.1.input_layernorm.weight torch.float16\n",
      "model.layers.1.post_attention_layernorm.weight torch.float16\n",
      "model.layers.2.self_attn.q_proj.weight torch.float16\n",
      "model.layers.2.self_attn.k_proj.weight torch.float16\n",
      "model.layers.2.self_attn.v_proj.weight torch.float16\n",
      "model.layers.2.self_attn.o_proj.weight torch.float16\n",
      "model.layers.2.mlp.gate_proj.weight torch.float16\n",
      "model.layers.2.mlp.up_proj.weight torch.float16\n",
      "model.layers.2.mlp.down_proj.weight torch.float16\n",
      "model.layers.2.input_layernorm.weight torch.float16\n",
      "model.layers.2.post_attention_layernorm.weight torch.float16\n",
      "model.layers.3.self_attn.q_proj.weight torch.float16\n",
      "model.layers.3.self_attn.k_proj.weight torch.float16\n",
      "model.layers.3.self_attn.v_proj.weight torch.float16\n",
      "model.layers.3.self_attn.o_proj.weight torch.float16\n",
      "model.layers.3.mlp.gate_proj.weight torch.float16\n",
      "model.layers.3.mlp.up_proj.weight torch.float16\n",
      "model.layers.3.mlp.down_proj.weight torch.float16\n",
      "model.layers.3.input_layernorm.weight torch.float16\n",
      "model.layers.3.post_attention_layernorm.weight torch.float16\n",
      "model.layers.4.self_attn.q_proj.weight torch.float16\n",
      "model.layers.4.self_attn.k_proj.weight torch.float16\n",
      "model.layers.4.self_attn.v_proj.weight torch.float16\n",
      "model.layers.4.self_attn.o_proj.weight torch.float16\n",
      "model.layers.4.mlp.gate_proj.weight torch.float16\n",
      "model.layers.4.mlp.up_proj.weight torch.float16\n",
      "model.layers.4.mlp.down_proj.weight torch.float16\n",
      "model.layers.4.input_layernorm.weight torch.float16\n",
      "model.layers.4.post_attention_layernorm.weight torch.float16\n",
      "model.layers.5.self_attn.q_proj.weight torch.float16\n",
      "model.layers.5.self_attn.k_proj.weight torch.float16\n",
      "model.layers.5.self_attn.v_proj.weight torch.float16\n",
      "model.layers.5.self_attn.o_proj.weight torch.float16\n",
      "model.layers.5.mlp.gate_proj.weight torch.float16\n",
      "model.layers.5.mlp.up_proj.weight torch.float16\n",
      "model.layers.5.mlp.down_proj.weight torch.float16\n",
      "model.layers.5.input_layernorm.weight torch.float16\n",
      "model.layers.5.post_attention_layernorm.weight torch.float16\n",
      "model.layers.6.self_attn.q_proj.weight torch.float16\n",
      "model.layers.6.self_attn.k_proj.weight torch.float16\n",
      "model.layers.6.self_attn.v_proj.weight torch.float16\n",
      "model.layers.6.self_attn.o_proj.weight torch.float16\n",
      "model.layers.6.mlp.gate_proj.weight torch.float16\n",
      "model.layers.6.mlp.up_proj.weight torch.float16\n",
      "model.layers.6.mlp.down_proj.weight torch.float16\n",
      "model.layers.6.input_layernorm.weight torch.float16\n",
      "model.layers.6.post_attention_layernorm.weight torch.float16\n",
      "model.layers.7.self_attn.q_proj.weight torch.float16\n",
      "model.layers.7.self_attn.k_proj.weight torch.float16\n",
      "model.layers.7.self_attn.v_proj.weight torch.float16\n",
      "model.layers.7.self_attn.o_proj.weight torch.float16\n",
      "model.layers.7.mlp.gate_proj.weight torch.float16\n",
      "model.layers.7.mlp.up_proj.weight torch.float16\n",
      "model.layers.7.mlp.down_proj.weight torch.float16\n",
      "model.layers.7.input_layernorm.weight torch.float16\n",
      "model.layers.7.post_attention_layernorm.weight torch.float16\n",
      "model.layers.8.self_attn.q_proj.weight torch.float16\n",
      "model.layers.8.self_attn.k_proj.weight torch.float16\n",
      "model.layers.8.self_attn.v_proj.weight torch.float16\n",
      "model.layers.8.self_attn.o_proj.weight torch.float16\n",
      "model.layers.8.mlp.gate_proj.weight torch.float16\n",
      "model.layers.8.mlp.up_proj.weight torch.float16\n",
      "model.layers.8.mlp.down_proj.weight torch.float16\n",
      "model.layers.8.input_layernorm.weight torch.float16\n",
      "model.layers.8.post_attention_layernorm.weight torch.float16\n",
      "model.layers.9.self_attn.q_proj.weight torch.float16\n",
      "model.layers.9.self_attn.k_proj.weight torch.float16\n",
      "model.layers.9.self_attn.v_proj.weight torch.float16\n",
      "model.layers.9.self_attn.o_proj.weight torch.float16\n",
      "model.layers.9.mlp.gate_proj.weight torch.float16\n",
      "model.layers.9.mlp.up_proj.weight torch.float16\n",
      "model.layers.9.mlp.down_proj.weight torch.float16\n",
      "model.layers.9.input_layernorm.weight torch.float16\n",
      "model.layers.9.post_attention_layernorm.weight torch.float16\n",
      "model.layers.10.self_attn.q_proj.weight torch.float16\n",
      "model.layers.10.self_attn.k_proj.weight torch.float16\n",
      "model.layers.10.self_attn.v_proj.weight torch.float16\n",
      "model.layers.10.self_attn.o_proj.weight torch.float16\n",
      "model.layers.10.mlp.gate_proj.weight torch.float16\n",
      "model.layers.10.mlp.up_proj.weight torch.float16\n",
      "model.layers.10.mlp.down_proj.weight torch.float16\n",
      "model.layers.10.input_layernorm.weight torch.float16\n",
      "model.layers.10.post_attention_layernorm.weight torch.float16\n",
      "model.layers.11.self_attn.q_proj.weight torch.float16\n",
      "model.layers.11.self_attn.k_proj.weight torch.float16\n",
      "model.layers.11.self_attn.v_proj.weight torch.float16\n",
      "model.layers.11.self_attn.o_proj.weight torch.float16\n",
      "model.layers.11.mlp.gate_proj.weight torch.float16\n",
      "model.layers.11.mlp.up_proj.weight torch.float16\n",
      "model.layers.11.mlp.down_proj.weight torch.float16\n",
      "model.layers.11.input_layernorm.weight torch.float16\n",
      "model.layers.11.post_attention_layernorm.weight torch.float16\n",
      "model.layers.12.self_attn.q_proj.weight torch.float16\n",
      "model.layers.12.self_attn.k_proj.weight torch.float16\n",
      "model.layers.12.self_attn.v_proj.weight torch.float16\n",
      "model.layers.12.self_attn.o_proj.weight torch.float16\n",
      "model.layers.12.mlp.gate_proj.weight torch.float16\n",
      "model.layers.12.mlp.up_proj.weight torch.float16\n",
      "model.layers.12.mlp.down_proj.weight torch.float16\n",
      "model.layers.12.input_layernorm.weight torch.float16\n",
      "model.layers.12.post_attention_layernorm.weight torch.float16\n",
      "model.layers.13.self_attn.q_proj.weight torch.float16\n",
      "model.layers.13.self_attn.k_proj.weight torch.float16\n",
      "model.layers.13.self_attn.v_proj.weight torch.float16\n",
      "model.layers.13.self_attn.o_proj.weight torch.float16\n",
      "model.layers.13.mlp.gate_proj.weight torch.float16\n",
      "model.layers.13.mlp.up_proj.weight torch.float16\n",
      "model.layers.13.mlp.down_proj.weight torch.float16\n",
      "model.layers.13.input_layernorm.weight torch.float16\n",
      "model.layers.13.post_attention_layernorm.weight torch.float16\n",
      "model.layers.14.self_attn.q_proj.weight torch.float16\n",
      "model.layers.14.self_attn.k_proj.weight torch.float16\n",
      "model.layers.14.self_attn.v_proj.weight torch.float16\n",
      "model.layers.14.self_attn.o_proj.weight torch.float16\n",
      "model.layers.14.mlp.gate_proj.weight torch.float16\n",
      "model.layers.14.mlp.up_proj.weight torch.float16\n",
      "model.layers.14.mlp.down_proj.weight torch.float16\n",
      "model.layers.14.input_layernorm.weight torch.float16\n",
      "model.layers.14.post_attention_layernorm.weight torch.float16\n",
      "model.layers.15.self_attn.q_proj.weight torch.float16\n",
      "model.layers.15.self_attn.k_proj.weight torch.float16\n",
      "model.layers.15.self_attn.v_proj.weight torch.float16\n",
      "model.layers.15.self_attn.o_proj.weight torch.float16\n",
      "model.layers.15.mlp.gate_proj.weight torch.float16\n",
      "model.layers.15.mlp.up_proj.weight torch.float16\n",
      "model.layers.15.mlp.down_proj.weight torch.float16\n",
      "model.layers.15.input_layernorm.weight torch.float16\n",
      "model.layers.15.post_attention_layernorm.weight torch.float16\n",
      "model.norm.weight torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    print(name, parameter.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Step1 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "\n",
    "# 可以使用qkv来指定，也可以用正则表达式进行指定\n",
    "# ['q', 'v'] or '.*decoder.*(SelfAttention|EncDecAttention).*(q|v)$'.\"\n",
    "# config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=\".*\\.1.*query_key_value\", modules_to_save=[\"word_embeddings\"])\n",
    "\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM)\n",
    "# config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"query_key_value\", \"dense_4h_to_h\"])\n",
    "# config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=\".*\\.1.*query_key_value\", modules_to_save=[\"word_embeddings\"])\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Step2 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.norm.weight torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    print(name, parameter.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='D:/pretrained_model/LLM-Research/Llama-3___2-1B', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'q_proj', 'v_proj'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient_checkpointing=True的情况下需要增加\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./chatbot_lora_kbit\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    gradient_checkpointing=True,\n",
    "    adam_epsilon=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49207\\AppData\\Local\\Temp\\ipykernel_10824\\1988846643.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dc02aefc3e4c8e82a132fac736c175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2542, 'grad_norm': 0.89013671875, 'learning_rate': 4.9702026221692494e-05, 'epoch': 0.01}\n",
      "{'loss': 2.1059, 'grad_norm': 1.009765625, 'learning_rate': 4.9404052443384986e-05, 'epoch': 0.01}\n",
      "{'loss': 2.0694, 'grad_norm': 0.970703125, 'learning_rate': 4.910607866507748e-05, 'epoch': 0.02}\n",
      "{'loss': 1.9958, 'grad_norm': 0.7880859375, 'learning_rate': 4.880810488676997e-05, 'epoch': 0.02}\n",
      "{'loss': 1.9855, 'grad_norm': 0.74072265625, 'learning_rate': 4.851013110846246e-05, 'epoch': 0.03}\n",
      "{'loss': 2.0075, 'grad_norm': 0.78173828125, 'learning_rate': 4.821215733015495e-05, 'epoch': 0.04}\n",
      "{'loss': 2.0328, 'grad_norm': 0.81884765625, 'learning_rate': 4.791418355184744e-05, 'epoch': 0.04}\n",
      "{'loss': 2.0383, 'grad_norm': 0.85888671875, 'learning_rate': 4.7616209773539935e-05, 'epoch': 0.05}\n",
      "{'loss': 2.0133, 'grad_norm': 0.888671875, 'learning_rate': 4.7318235995232426e-05, 'epoch': 0.05}\n",
      "{'loss': 2.0092, 'grad_norm': 0.9296875, 'learning_rate': 4.702026221692492e-05, 'epoch': 0.06}\n",
      "{'loss': 1.9751, 'grad_norm': 0.8046875, 'learning_rate': 4.672228843861741e-05, 'epoch': 0.07}\n",
      "{'loss': 2.0135, 'grad_norm': 0.95458984375, 'learning_rate': 4.6424314660309894e-05, 'epoch': 0.07}\n",
      "{'loss': 2.0386, 'grad_norm': 1.2177734375, 'learning_rate': 4.6126340882002386e-05, 'epoch': 0.08}\n",
      "{'loss': 1.9998, 'grad_norm': 0.93603515625, 'learning_rate': 4.582836710369488e-05, 'epoch': 0.08}\n",
      "{'loss': 2.0628, 'grad_norm': 0.990234375, 'learning_rate': 4.553039332538737e-05, 'epoch': 0.09}\n",
      "{'loss': 1.9785, 'grad_norm': 0.89990234375, 'learning_rate': 4.523241954707986e-05, 'epoch': 0.1}\n",
      "{'loss': 1.8832, 'grad_norm': 0.8798828125, 'learning_rate': 4.4934445768772345e-05, 'epoch': 0.1}\n",
      "{'loss': 2.0059, 'grad_norm': 0.7548828125, 'learning_rate': 4.463647199046484e-05, 'epoch': 0.11}\n",
      "{'loss': 2.0058, 'grad_norm': 0.89013671875, 'learning_rate': 4.433849821215733e-05, 'epoch': 0.11}\n",
      "{'loss': 2.0035, 'grad_norm': 0.95703125, 'learning_rate': 4.404052443384982e-05, 'epoch': 0.12}\n",
      "{'loss': 1.9791, 'grad_norm': 0.93017578125, 'learning_rate': 4.374255065554231e-05, 'epoch': 0.13}\n",
      "{'loss': 2.0268, 'grad_norm': 0.8154296875, 'learning_rate': 4.34445768772348e-05, 'epoch': 0.13}\n",
      "{'loss': 1.9208, 'grad_norm': 0.828125, 'learning_rate': 4.3146603098927295e-05, 'epoch': 0.14}\n",
      "{'loss': 1.9625, 'grad_norm': 0.7099609375, 'learning_rate': 4.2848629320619786e-05, 'epoch': 0.14}\n",
      "{'loss': 2.0342, 'grad_norm': 0.783203125, 'learning_rate': 4.255065554231228e-05, 'epoch': 0.15}\n",
      "{'loss': 1.982, 'grad_norm': 0.7666015625, 'learning_rate': 4.225268176400477e-05, 'epoch': 0.15}\n",
      "{'loss': 1.9673, 'grad_norm': 0.9853515625, 'learning_rate': 4.195470798569726e-05, 'epoch': 0.16}\n",
      "{'loss': 1.9811, 'grad_norm': 0.974609375, 'learning_rate': 4.165673420738975e-05, 'epoch': 0.17}\n",
      "{'loss': 1.9913, 'grad_norm': 1.0439453125, 'learning_rate': 4.1358760429082244e-05, 'epoch': 0.17}\n",
      "{'loss': 1.948, 'grad_norm': 0.79296875, 'learning_rate': 4.1060786650774736e-05, 'epoch': 0.18}\n",
      "{'loss': 1.9813, 'grad_norm': 1.2978515625, 'learning_rate': 4.076281287246723e-05, 'epoch': 0.18}\n",
      "{'loss': 1.9488, 'grad_norm': 0.7958984375, 'learning_rate': 4.046483909415972e-05, 'epoch': 0.19}\n",
      "{'loss': 1.9207, 'grad_norm': 0.734375, 'learning_rate': 4.016686531585221e-05, 'epoch': 0.2}\n",
      "{'loss': 1.9462, 'grad_norm': 0.93212890625, 'learning_rate': 3.98688915375447e-05, 'epoch': 0.2}\n",
      "{'loss': 1.932, 'grad_norm': 0.8046875, 'learning_rate': 3.9570917759237194e-05, 'epoch': 0.21}\n",
      "{'loss': 1.9581, 'grad_norm': 0.712890625, 'learning_rate': 3.927294398092968e-05, 'epoch': 0.21}\n",
      "{'loss': 2.0035, 'grad_norm': 0.89794921875, 'learning_rate': 3.897497020262217e-05, 'epoch': 0.22}\n",
      "{'loss': 1.902, 'grad_norm': 0.9521484375, 'learning_rate': 3.867699642431466e-05, 'epoch': 0.23}\n",
      "{'loss': 1.9184, 'grad_norm': 0.93212890625, 'learning_rate': 3.837902264600715e-05, 'epoch': 0.23}\n",
      "{'loss': 2.0269, 'grad_norm': 0.91796875, 'learning_rate': 3.8081048867699645e-05, 'epoch': 0.24}\n",
      "{'loss': 1.8981, 'grad_norm': 0.6767578125, 'learning_rate': 3.7783075089392136e-05, 'epoch': 0.24}\n",
      "{'loss': 1.9756, 'grad_norm': 1.017578125, 'learning_rate': 3.748510131108463e-05, 'epoch': 0.25}\n",
      "{'loss': 1.9153, 'grad_norm': 0.8076171875, 'learning_rate': 3.718712753277712e-05, 'epoch': 0.26}\n",
      "{'loss': 2.0088, 'grad_norm': 0.81103515625, 'learning_rate': 3.688915375446961e-05, 'epoch': 0.26}\n",
      "{'loss': 1.9325, 'grad_norm': 0.8515625, 'learning_rate': 3.6591179976162096e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\49207\\.conda\\envs\\py311_langchainchat\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\49207\\.conda\\envs\\py311_langchainchat\\Lib\\site-packages\\transformers\\trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2522\u001b[0m )\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2530\u001b[0m ):\n\u001b[0;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\49207\\.conda\\envs\\py311_langchainchat\\Lib\\site-packages\\transformers\\trainer.py:3687\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3685\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3687\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3688\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\49207\\.conda\\envs\\py311_langchainchat\\Lib\\site-packages\\accelerate\\accelerator.py:2248\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2248\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 熬夜后第二天有哪些方法可以修复身体？中文回复我\n",
      "\n",
      "Assistant: 在熬夜后第二天一般需要进行修复身体的方法。这些方法可以包括饮食、休息、运动、锻炼等。这些方法可以帮助身体恢复健康，减轻疲劳，并促进肌肉的发育。建议在熬夜后第二天进行这些方法，帮助身体恢复健康。\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"熬夜后第二天有哪些方法可以修复身体？中文回复我\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\").to(model.device)\n",
    "print(tokenizer.decode(model.generate(**ipt, max_length=256, do_sample=True)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.tensor(1e-8).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_langchainchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
